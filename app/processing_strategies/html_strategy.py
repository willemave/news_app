"""
This module defines the strategy for processing standard HTML web pages using crawl4ai.
"""

import asyncio
import logging
import re
from html import unescape
from typing import Any

import httpx
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CacheMode,
    CrawlerRunConfig,
    LLMConfig,
    LLMTableExtraction,
)
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

from app.core.logging import get_logger
from app.core.settings import get_settings
from app.http_client.robust_http_client import RobustHttpClient
from app.processing_strategies.base_strategy import UrlProcessorStrategy
from app.utils.dates import parse_date_with_tz
from app.utils.error_logger import create_error_logger

logger = get_logger(__name__)


class HtmlProcessorStrategy(UrlProcessorStrategy):
    """
    Strategy for processing standard HTML web pages.
    It downloads HTML content using crawl4ai with optimized content extraction,
    and prepares it for further processing.
    """

    def __init__(self, http_client: RobustHttpClient):
        super().__init__(http_client)
        self.error_logger = create_error_logger("html_strategy")
        self.settings = get_settings()

    def _detect_source(self, url: str) -> str:
        """Detect the source type from URL."""
        if "pubmed.ncbi.nlm.nih.gov" in url or "pmc.ncbi.nlm.nih.gov" in url:
            return "PubMed"
        elif "arxiv.org" in url:
            return "Arxiv"
        elif "substack.com" in url:
            return "Substack"
        elif "medium.com" in url:
            return "Medium"
        elif "chinatalk.media" in url:
            return "ChinaTalk"
        else:
            return "web"

    def _map_platform(self, source: str, url: str) -> str | None:
        """Map platform from the detected source or URL.

        Keeps platform taxonomy consistent with scrapers (substack, medium,
        arxiv, pubmed, youtube, etc.). Returns None for generic web.
        """
        s = (source or "").lower()
        if s == "substack":
            return "substack"
        if s == "medium":
            return "medium"
        if s == "arxiv":
            return "arxiv"
        if s in ("pubmed", "pmc"):
            return "pubmed"
        # Some Substack publications use custom domains (e.g., chinatalk.media)
        if any(h in url for h in (".substack.com", "chinatalk.media")):
            return "substack"
        # Fallback: no clear platform
        return None

    def preprocess_url(self, url: str) -> str:
        """
        Preprocess URLs to ensure we get the full content.
        - Transform PubMed URLs to PMC full-text URLs
        - Transform ArXiv abstract URLs to PDF URLs
        """
        # Handle PubMed URLs - transform to PMC full-text if available
        pubmed_match = re.match(r"https?://pubmed\.ncbi\.nlm\.nih\.gov/(\d+)", url)
        if pubmed_match:
            pmid = pubmed_match.group(1)
            pmc_url = f"https://pmc.ncbi.nlm.nih.gov/articles/pmid/{pmid}/"
            logger.debug("HtmlStrategy: Transforming PubMed URL %s to PMC URL %s", url, pmc_url)
            return pmc_url

        # Handle ArXiv URLs - transform abstract to PDF
        if "arxiv.org/abs/" in url:
            logger.debug("HtmlStrategy: Transforming arXiv URL %s", url)
            return url.replace("/abs/", "/pdf/")

        logger.debug(
            "HtmlStrategy: preprocess_url called for %s, no transformation applied.",
            url,
        )
        return url

    def can_handle_url(self, url: str, response_headers: httpx.Headers | None = None) -> bool:
        """
        Determines if this strategy can handle the given URL.
        Checks for 'text/html' in Content-Type or common HTML file extensions.
        """
        if response_headers:
            content_type = response_headers.get("content-type", "").lower()
            if "text/html" in content_type:
                logger.debug(
                    "HtmlStrategy can handle %s based on Content-Type: %s",
                    url,
                    content_type,
                )
                return True

        # Fallback: check URL pattern if no headers (e.g. direct call without HEAD)
        if not url.lower().endswith((".pdf", ".xml", ".json", ".txt")) and url.lower().startswith(
            ("http://", "https://")
        ):
            # ArXiv PDF URLs are handled by ArxivStrategy or PdfStrategy.
            # This check ensures HtmlStrategy doesn't mistakenly claim them.
            if "arxiv.org/pdf/" in url.lower():
                logger.debug(
                    f"HtmlStrategy: URL {url} appears to be an arXiv PDF, "
                    "deferring to other strategies."
                )
                return False
            logger.debug(
                f"HtmlStrategy attempting to handle {url} based on URL pattern "
                "(not PDF/XML/JSON/TXT)."
            )
            return True  # A bit of a catch-all if no other strategy matches

        logger.debug(f"HtmlStrategy cannot handle {url} based on current checks.")
        return False

    def download_content(self, url: str) -> str:
        """
        Downloads HTML content from the given URL.
        For crawl4ai, we'll use the extract_data method directly since it handles downloading.
        This method remains for compatibility with the base class.
        """
        logger.info(f"HtmlStrategy: download_content called for {url}")
        # We'll actually download in extract_data using crawl4ai
        return url  # Return the URL itself as a placeholder

    def _get_source_specific_config(self, source: str) -> dict[str, Any]:
        """Get source-specific configuration for crawl4ai."""
        # Base configuration
        config = {
            "word_count_threshold": 20,
            "excluded_tags": ["script", "style", "nav", "footer", "header"],
            "exclude_external_links": True,
            "remove_overlay_elements": True,
            "page_timeout_ms": 90_000,
            "wait_for_timeout_ms": 90_000,
            "max_crawl_attempts": 1,
            "crawl_retry_delay_seconds": 1.5,
        }

        # Source-specific adjustments
        if source == "Substack":
            config["excluded_tags"].extend(["form", "aside"])
            config["excluded_selector"] = (
                ".subscribe-widget, .footer-wrap, .subscription-form-wrapper"
            )
            config["target_elements"] = [".post", ".post-content", "article"]
            config["max_crawl_attempts"] = 2
            config["page_timeout_ms"] = 120_000
            config["wait_for_timeout_ms"] = 120_000
        elif source == "Medium":
            config["excluded_selector"] = ".metabar, .js-postActions, .js-stickyFooter"
            config["target_elements"] = ["article", ".postArticle", ".section-content"]
        elif source in ["PubMed", "PMC"]:
            # Keep more scientific content
            config["excluded_tags"] = ["script", "style", "nav", "footer"]
            config["target_elements"] = [".article", ".abstract", ".body", ".content", "main"]
            config["word_count_threshold"] = 10  # Lower threshold for scientific content
        elif source == "ChinaTalk":
            config["target_elements"] = [".post-content", ".post", "article"]
            config["excluded_selector"] = ".subscribe-widget, .comments-section"
            config["max_crawl_attempts"] = 2
            config["page_timeout_ms"] = 120_000
            config["wait_for_timeout_ms"] = 120_000
        elif source == "Arxiv":
            # ArXiv PDFs need special handling
            config["pdf"] = True

        return config

    def _resolve_llm_api_token(self, provider: str) -> str | None:
        """Resolve the API token to use for the configured LLM provider."""
        provider_name = provider.split("/", 1)[0].lower()
        if provider_name == "openai":
            return self.settings.openai_api_key
        if provider_name == "google":
            return self.settings.google_api_key
        if provider_name in {"anthropic", "claude"}:
            return self.settings.anthropic_api_key
        return None

    def _build_table_extraction_strategy(self) -> LLMTableExtraction | None:
        """Create an optional table extraction strategy for crawl4ai."""
        if not getattr(self.settings, "crawl4ai_enable_table_extraction", False):
            return None

        provider = getattr(self.settings, "crawl4ai_table_provider", None)
        if not provider:
            logger.debug("crawl4ai table extraction enabled but provider not configured")
            return None

        api_token = self._resolve_llm_api_token(provider)
        llm_config_kwargs: dict[str, Any] = {"provider": provider}
        if api_token:
            llm_config_kwargs["api_token"] = api_token

        css_selector = getattr(self.settings, "crawl4ai_table_css_selector", None)
        if css_selector:
            css_selector = css_selector.strip() or None

        try:
            return LLMTableExtraction(
                llm_config=LLMConfig(**llm_config_kwargs),
                css_selector=css_selector,
                enable_chunking=getattr(self.settings, "crawl4ai_table_enable_chunking", True),
                chunk_token_threshold=getattr(
                    self.settings, "crawl4ai_table_chunk_token_threshold", 3000
                ),
                min_rows_per_chunk=getattr(self.settings, "crawl4ai_table_min_rows_per_chunk", 10),
                max_parallel_chunks=getattr(self.settings, "crawl4ai_table_max_parallel_chunks", 5),
                verbose=getattr(self.settings, "crawl4ai_table_verbose", False),
            )
        except Exception as exc:  # pragma: no cover - defensive logging
            logger.warning("Failed to initialize crawl4ai table extraction strategy: %s", exc)
            return None

    @staticmethod
    def _is_retryable_crawl_error(error: Exception) -> bool:
        """Return True when the crawl error looks transient and merits a retry."""

        message = str(error).lower()
        transient_tokens = [
            "net::err_timed_out",
            "timeout",
            "wait condition failed",
            "selector 'body'",
            "net::err_connection_refused",
            "net::err_http2_protocol_error",
            "net::err_cert_verifier_changed",
            "net::err_connection_reset",
            "net::err_failed",
        ]
        return any(token in message for token in transient_tokens)

    @staticmethod
    def _should_use_httpx_fallback(error: Exception) -> bool:
        """Return True when a lightweight fetch/parse fallback is worth trying."""

        message = str(error).lower()
        fallback_tokens = [
            "net::err_connection_refused",
            "net::err_http2_protocol_error",
            "net::err_cert_verifier_changed",
            "wait condition failed",
            "timeout after",
        ]
        return any(token in message for token in fallback_tokens)

    @staticmethod
    def _extract_title_from_html(html_content: str) -> str | None:
        """Extract a page title from raw HTML."""

        match = re.search(r"<title[^>]*>(.*?)</title>", html_content, re.IGNORECASE | re.DOTALL)
        if not match:
            return None
        title = unescape(match.group(1)).strip()
        return re.sub(r"\s+", " ", title) if title else None

    @staticmethod
    def _extract_text_from_html(html_content: str) -> str:
        """Lightweight HTML to text extraction for fallback."""

        without_scripts = re.sub(r"(?is)<(script|style).*?>.*?</\\1>", " ", html_content)
        without_tags = re.sub(r"(?is)<[^>]+>", " ", without_scripts)
        text = unescape(without_tags)
        return re.sub(r"\s+", " ", text).strip()

    def _fallback_fetch(self, url: str, source: str) -> dict[str, Any] | None:
        """Use httpx + lightweight parsing when Playwright navigation fails."""

        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
        }
        try:
            response = httpx.get(url, headers=headers, timeout=20.0, follow_redirects=True)
            response.raise_for_status()
        except Exception as exc:  # noqa: BLE001
            logger.error("HtmlStrategy fallback fetch failed for %s: %s", url, exc)
            return None

        html_content = response.text
        title = self._extract_title_from_html(html_content) or "Untitled"
        text_content = self._extract_text_from_html(html_content)
        logger.info(
            "HtmlStrategy: Fallback extraction succeeded for %s (text_length=%s)",
            response.url,
            len(text_content),
        )
        return {
            "title": title,
            "author": None,
            "publication_date": None,
            "text_content": text_content,
            "content_type": "html",
            "source": source,
            "final_url_after_redirects": str(response.url),
            "table_markdown": None,
        }

    def extract_data(self, content: str, url: str) -> dict[str, Any]:
        """
        Extracts data from HTML content using crawl4ai.
        'content' parameter is ignored as crawl4ai handles downloading.
        'url' here is the final URL after any preprocessing.
        """
        logger.info(f"HtmlStrategy: Extracting data from {url}")

        # Detect source for metadata
        source = self._detect_source(url)
        logger.debug("HtmlStrategy: Starting extraction (url=%s, source=%s)", url, source)
        table_strategy = self._build_table_extraction_strategy()

        try:
            # Configure browser
            browser_config = BrowserConfig(
                headless=True,
                viewport_width=1920,
                viewport_height=1080,
                text_mode=False,
                light_mode=True,
                ignore_https_errors=True,
                java_script_enabled=True,
                extra_args=["--disable-blink-features=AutomationControlled"],
                verbose=False,
            )

            # Get source-specific configuration
            source_config = self._get_source_specific_config(source)
            page_timeout_ms = int(source_config.get("page_timeout_ms", 90_000))
            wait_for_timeout_ms = int(source_config.get("wait_for_timeout_ms", page_timeout_ms))
            max_crawl_attempts = max(1, int(source_config.get("max_crawl_attempts", 3)))
            retry_delay_seconds = float(source_config.get("crawl_retry_delay_seconds", 1.5))

            # Configure crawler run
            run_config = CrawlerRunConfig(
                # Content filtering
                word_count_threshold=source_config.get("word_count_threshold", 20),
                excluded_tags=source_config.get("excluded_tags", []),
                excluded_selector=source_config.get("excluded_selector"),
                target_elements=source_config.get("target_elements"),
                exclude_external_links=source_config.get("exclude_external_links", True),
                # Content processing
                process_iframes=False,
                remove_overlay_elements=source_config.get("remove_overlay_elements", True),
                remove_forms=True,
                keep_data_attributes=False,
                # Page handling
                wait_until="domcontentloaded",
                wait_for="body",
                delay_before_return_html=1.0,
                page_timeout=page_timeout_ms,
                wait_for_timeout=wait_for_timeout_ms,
                adjust_viewport_to_content=True,
                # Performance
                cache_mode=CacheMode.BYPASS,
                verbose=False,
                # Link filtering
                exclude_social_media_links=True,
                exclude_domains=["facebook.com", "twitter.com", "instagram.com", "linkedin.com"],
                # Special handling
                pdf=source_config.get("pdf", False),
                check_robots_txt=False,
                table_extraction=table_strategy,
                markdown_generator=DefaultMarkdownGenerator(
                    content_source="raw_html",
                    options={
                        "ignore_links": False,
                        "ignore_images": True,  # Avoid Base64 data URIs bloating content
                        "escape_html": False,
                        "body_width": 0,
                    },
                ),
            )
            logger.debug(
                "HtmlStrategy: Crawl config prepared "
                "(url=%s, word_count_threshold=%s, target_elements=%s)",
                url,
                run_config.word_count_threshold,
                run_config.target_elements,
            )

            # Use AsyncWebCrawler with asyncio.run
            async def run_crawl_with_retries():
                crawl4ai_logger = logging.getLogger("crawl4ai")
                original_level = crawl4ai_logger.level
                crawl4ai_logger.setLevel(logging.WARNING)
                try:
                    last_error: Exception | None = None
                    for attempt in range(1, max_crawl_attempts + 1):
                        crawler = None
                        should_retry = False
                        try:
                            crawler = AsyncWebCrawler(config=browser_config)
                            await crawler.__aenter__()
                            result = await crawler.arun(url=url, config=run_config)
                            logger.debug(
                                "HtmlStrategy: Crawl finished "
                                "(url=%s, success=%s, status=%s, redirected=%s)",
                                url,
                                getattr(result, "success", None),
                                getattr(result, "status_code", None),
                                getattr(result, "redirected_url", None),
                            )
                            return result
                        except Exception as exc:  # noqa: BLE001
                            last_error = exc
                            logger.debug(
                                "HtmlStrategy: Crawl attempt %s/%s failed for %s: %s",
                                attempt,
                                max_crawl_attempts,
                                url,
                                exc,
                            )
                            if self._is_retryable_crawl_error(exc) and attempt < max_crawl_attempts:
                                should_retry = True
                                logger.warning(
                                    "HtmlStrategy: Retrying crawl for %s after timeout "
                                    "(attempt %s/%s)",
                                    url,
                                    attempt + 1,
                                    max_crawl_attempts,
                                )
                            else:
                                raise
                        finally:
                            if crawler:
                                try:
                                    await crawler.__aexit__(None, None, None)
                                except Exception as close_error:  # noqa: BLE001
                                    # Log browser close errors but don't fail the extraction
                                    logger.debug(
                                        "Error closing browser (non-critical): %s", close_error
                                    )
                        if should_retry:
                            await asyncio.sleep(retry_delay_seconds)

                    if last_error is not None:
                        raise last_error
                    raise RuntimeError("Crawl4ai retry loop exited without result")
                finally:
                    crawl4ai_logger.setLevel(original_level)

            try:
                result = asyncio.run(run_crawl_with_retries())
            except Exception as crawl_exc:  # noqa: BLE001
                if self._should_use_httpx_fallback(crawl_exc):
                    fallback_data = self._fallback_fetch(url, source)
                    if fallback_data:
                        return fallback_data
                raise

            # Check if result is None
            if result is None:
                error_msg = "Crawl4ai extraction returned None - possible timeout or network issue"
                logger.warning(f"{error_msg} for URL: {url}")
                raise Exception(error_msg)

            if not result.success:
                error_detail = getattr(result, "error_message", None) or getattr(
                    result, "error", None
                )

                if not error_detail:
                    # Some crawl4ai failures surface an `errors` list
                    errors = getattr(result, "errors", None)
                    if errors:
                        error_detail = "; ".join(str(e) for e in errors if e)

                if not error_detail:
                    error_detail = "Unknown error"

                status_code = getattr(result, "status_code", None)
                if status_code:
                    error_detail = f"{error_detail} (status_code={status_code})"

                redirected_url = getattr(result, "redirected_url", None)
                if redirected_url and redirected_url != url:
                    error_detail = f"{error_detail} [redirected to {redirected_url}]"

                error_msg = f"Crawl4ai extraction failed: {error_detail}"
                logger.warning(f"{error_msg} for URL: {url}")

                if self._should_use_httpx_fallback(RuntimeError(error_detail)):
                    fallback_data = self._fallback_fetch(url, source)
                    if fallback_data:
                        return fallback_data

                raise Exception(error_msg)

            # Extract metadata from content if not provided
            extracted_text = result.markdown.raw_markdown if result.markdown else ""
            if not extracted_text:
                raise Exception("No content extracted from the page")
            logger.debug(
                "HtmlStrategy: Extracted markdown length=%s cleaned_html_length=%s",
                len(extracted_text),
                len(result.cleaned_html or ""),
            )
            logger.debug(
                "HtmlStrategy: Markdown preview: %s",
                (extracted_text[:200] + "...") if len(extracted_text) > 200 else extracted_text,
            )
            if result.cleaned_html:
                logger.debug(
                    "HtmlStrategy: Cleaned HTML preview: %s",
                    (
                        result.cleaned_html[:200].replace("\n", " ") + "..."
                        if len(result.cleaned_html) > 200
                        else result.cleaned_html.replace("\n", " ")
                    ),
                )
            if result.metadata:
                logger.debug("HtmlStrategy: Raw metadata keys=%s", list(result.metadata.keys()))

            title = (result.metadata.get("title") if result.metadata else None) or "Untitled"
            author = None
            publication_date = None
            table_markdown: list[str] = []

            if table_strategy and getattr(result, "tables", None):
                for table in result.tables or []:
                    table_md = getattr(table, "markdown", None)
                    if table_md:
                        table_markdown.append(table_md.strip())

            # Try to extract metadata from the content
            if extracted_text:
                # Simple pattern matching for common metadata patterns
                # Author patterns
                author_patterns = [
                    r"(?:By|Author|Written by)[:\s]+([^\n]+)",
                    r"<meta[^>]+name=[\"']author[\"'][^>]+content=[\"']([^\"']+)[\"']",
                ]

                # First check cleaned HTML for meta tags
                cleaned_html = result.cleaned_html if hasattr(result, "cleaned_html") else ""
                if cleaned_html:
                    for pattern in author_patterns[1:]:  # Meta tag patterns
                        match = re.search(pattern, cleaned_html, re.IGNORECASE)
                        if match:
                            author = match.group(1).strip()
                            break

                # Then check markdown content
                if not author:
                    for pattern in author_patterns[:1]:  # Text patterns
                        match = re.search(pattern, extracted_text, re.IGNORECASE)
                        if match:
                            author = match.group(1).strip()
                            # Clean up author if it contains extra content
                            if len(author) > 100:  # Likely grabbed too much
                                author = None
                            break

                # Date patterns
                date_patterns = [
                    r"(?:Published|Date|Posted)[:\s]+([^\n]+\d{4}[^\n]*)",
                    r"(\d{1,2}[-/]\d{1,2}[-/]\d{2,4})",
                    r"(\d{4}[-/]\d{1,2}[-/]\d{1,2})",
                ]
                for pattern in date_patterns:
                    match = re.search(pattern, extracted_text, re.IGNORECASE)
                    if match:
                        date_str = match.group(1).strip()
                        publication_date = parse_date_with_tz(date_str)
                        if publication_date:
                            break

            logger.info(
                f"HtmlStrategy: Successfully extracted data for {url}. "
                f"Title: {title[:50] if title else 'None'}... Source: {source}"
            )
            # Map source to full domain name of final URL
            try:
                from urllib.parse import urlparse

                final_url = result.url if hasattr(result, "url") and result.url else url
                host = urlparse(final_url).netloc or ""
            except Exception:
                final_url = url
                host = ""
            logger.debug(
                "HtmlStrategy: Extraction metadata (final_url=%s, publication_date=%s, author=%s)",
                final_url,
                publication_date,
                author,
            )
            return {
                "title": title,
                "author": author,
                "publication_date": publication_date,
                "text_content": extracted_text,
                "content_type": "html",
                # Source should be full domain name, leave platform to the scraper convention
                "source": host,
                "final_url_after_redirects": final_url,
                "table_markdown": table_markdown or None,
            }

        except Exception as e:
            import traceback

            from app.services.http import NonRetryableError

            if self._should_use_httpx_fallback(e):
                fallback_data = self._fallback_fetch(url, source)
                if fallback_data:
                    logger.warning(
                        "HtmlStrategy: Using fallback extraction for %s after error: %s", url, e
                    )
                    return fallback_data

            error_msg = f"Content extraction failed for {url}: {str(e)}"
            traceback_str = traceback.format_exc()

            # Log the error
            self.error_logger.log_processing_error(
                item_id=url,
                error=e,
                operation="html_content_extraction",
                context={
                    "url": url,
                    "strategy": "html",
                    "source": source,
                    "method": "crawl4ai",
                    "traceback": traceback_str,
                },
            )
            logger.error(
                "HtmlStrategy: %s\nTraceback: %s",
                error_msg,
                traceback_str,
            )

            # Check if this is a non-retryable error
            error_str = str(e).lower()
            non_retryable_terms = [
                "403",
                "401",
                "404",
                "blocked",
                "forbidden",
                "access denied",
                "not found",
                "paywall",
                "err_http_response_code_failure",
                "err_http2_protocol_error",
                "err_ssl_protocol_error",
                "err_connection_refused",
                "err_cert_",  # Catches various certificate errors
                "timeout",  # Page load timeouts indicate site issues
                "wait condition failed",  # Crawl4ai selector wait failures
            ]
            if any(term in error_str for term in non_retryable_terms):
                # Raise NonRetryableError to prevent infinite retries
                raise NonRetryableError(f"Non-retryable error: {error_msg}") from e

            # For other errors, return a minimal response to allow processing to continue
            # with fallback content
            # Failure path: still try to emit domain for source
            try:
                from urllib.parse import urlparse

                host = urlparse(url).netloc or ""
            except Exception:
                host = ""
            return {
                "title": f"Content from {url}",
                "text_content": f"Failed to extract content from {url}. Error: {str(e)}",
                "content_type": "html",
                "source": host,
                "final_url_after_redirects": url,
                "extraction_error": str(e),
            }

    def prepare_for_llm(self, extracted_data: dict[str, Any]) -> dict[str, Any]:
        """
        Prepares extracted HTML data for LLM processing.
        """
        logger.info(
            f"HtmlStrategy: Preparing data for LLM for URL: "
            f"{extracted_data.get('final_url_after_redirects')}"
        )
        text_content = extracted_data.get("text_content", "") or ""
        logger.debug("HtmlStrategy: LLM preparation payload length=%s", len(text_content))

        table_markdown = extracted_data.get("table_markdown")
        if table_markdown:
            if isinstance(table_markdown, list):
                combined_tables = "\n\n".join(
                    table for table in table_markdown if isinstance(table, str) and table
                )
            else:
                combined_tables = str(table_markdown)

            if combined_tables:
                text_content = (
                    f"{text_content}\n\n## Extracted Tables\n{combined_tables}"
                    if text_content
                    else f"## Extracted Tables\n{combined_tables}"
                )

        # Based on app.llm.py, filter_article and summarize_article take the content string.
        return {
            "content_to_filter": text_content,
            "content_to_summarize": text_content,
            "is_pdf": False,
        }

    def extract_internal_urls(self, content: str, original_url: str) -> list[str]:
        """
        Extracts internal URLs from HTML content for logging.
        This is a basic implementation; more sophisticated parsing might be needed.
        """
        # This is a placeholder. A more robust implementation would use BeautifulSoup
        # or a regex designed for URLs, and properly resolve relative URLs.
        logger.info(
            f"HtmlStrategy: extract_internal_urls called for {original_url}. "
            "(Placeholder - returning empty list)"
        )
        return []
