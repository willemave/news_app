<p><em>This post is authored by Sayash Kapoor, Rishi Bommasani, Daniel E. Ho, Percy Liang, and Arvind Narayanan. The paper has 25 authors listed <a href="https://crfm.stanford.edu/open-fms/#authors">here</a>.</em></p><p>Last October, President Biden signed the <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">Executive Order on Safe, Secure, and Trustworthy AI</a>. It tasked the National Telecommunications and Information Administration (NTIA) with preparing a report on the benefits and risks of open foundation models&#8212;foundation models with widely available model weights (such as Meta's Llama 2 and Stability's Stable Diffusion). There is widespread disagreement about the impact of openness on society, which the NTIA must sort through. Last week, the NTIA <a href="https://www.ntia.gov/federal-register-notice/2024/dual-use-foundation-artificial-intelligence-models-widely-available">released</a> a list of over 50 questions to solicit public input on the benefits and risks of open foundation models. The responses the NTIA receives will inform its report, which will, in turn, influence U.S. policy on open foundation models.</p><p><strong>Today, we are releasing a paper <a href="https://crfm.stanford.edu/open-fms">on the societal impact of open foundation models</a>.</strong> We make three main contributions. First, we diagnose that the disagreement on the impact of openness results from a lack of precision in claims about its societal impact. Second, we analyze the benefits of open foundation models such as transparency, distribution of power, and enabling scientific research (including when open model weights are <em>not </em>enough to realize some benefits). Third, we offer a risk assessment framework for assessing the <em>marginal</em> risk of open foundation models compared to closed models or existing technology like web search on the internet.</p><p>The paper is the result of a collaboration with 25 <a href="https://crfm.stanford.edu/open-fms/#authors">authors</a> across 16 academic, industry, and civil society organizations. Our aim is to bring clarity to pressing questions about how foundation models should be released and propose paths forward for researchers, developers, and policymakers.</p><h4><strong>Background: The debate on open foundation models</strong></h4><p>One of the biggest tech policy debates today is about how foundation models should be released. Access to some foundation models like OpenAI&#8217;s GPT-4 is limited to an API or a developer-provided product interface like ChatGPT. We call these models <em>closed.</em> Others, such as Meta&#8217;s Llama 2, are <em>open</em>, with widely available model weights enabling downstream modification and scrutiny. The <a href="https://www.ntia.gov/federal-register-notice/2024/dual-use-foundation-artificial-intelligence-models-widely-available">United States</a>, <a href="https://artificialintelligenceact.eu/article/52c/">EU</a>, and <a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute">UK</a> are all actively considering how to regulate open foundation models.</p><p>While there are <a href="https://crfm.stanford.edu/commentary/2021/10/18/sastry.html">many</a> <a href="https://crfm.stanford.edu/2022/05/17/community-norms.html">axes</a> involved in releasing models that form a <a href="https://arxiv.org/abs/2302.04844">gradient</a>, we focus on the dichotomy based on whether the weights are released widely. In particular, many of the risks described for open foundation models arise because developers relinquish control over who can use the model once it is released. For example, restrictions on what a model can be used for are both challenging to enforce and easy for malicious actors to ignore. In contrast, developers of closed foundation models can, in theory, reduce, restrict, or block access to their models. In short, the open release of model weights is irreversible.&nbsp;</p><p>As a result of this widespread access, some argue that widely available model weights could enable better research on their effects, promote competition and innovation, and improve scientific research, reproducibility, and transparency. Others argue that widely available model weights would enable malicious actors to more effectively misuse these models to generate disinformation, non-consensual intimate imagery, scams, and bioweapons.&nbsp;</p><h4><strong>Toward more precise analyses of societal impact</strong></h4><p>In October 2023, researchers from MIT released a <a href="https://arxiv.org/pdf/2310.18233.pdf">study</a> on the use of open language models for creating bioweapons. They looked at what information a malicious user might be able to find using open models that could aid the creation of pandemic-causing pathogens. Their findings were ominous, and their main recommendation to policymakers was to essentially ban open foundation models:&nbsp;</p><blockquote><p>Policymakers should recognize that it is not possible to stop third parties from removing safeguards from models with publicly accessible weights. Avoiding model weight proliferation appears to be necessary &#8211; but not sufficient &#8211; to prevent a future in which highly capable artificial intelligence can be abused to cause mass harm.&nbsp;</p></blockquote><p>While the paper focused on what information about building bioweapons users could find using language models, it did not compare it with information widely available on the internet. Follow-up studies from the <a href="https://www.rand.org/pubs/research_reports/RRA2977-2.html">RAND Institute</a> and <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation">OpenAI</a> focused on comparing the biosecurity risk from language models to information widely available on the internet. In stark contrast to the paper's claims, neither the RAND nor the OpenAI studies found that participants who used language models were significantly better than those who only used the internet to find information required to create bioweapons.</p><p>The MIT study is a cautionary tale of what can go wrong when analyses of the risks of open foundation models do not compare against risks from closed models or existing technology (such as web search on the internet). It also shows how in the absence of a clear methodology for analyzing the risks of open foundation models, researchers can end up talking past each other.&nbsp;</p><p>In our paper, we present a risk assessment framework to analyze the marginal risk of open foundation models. We focus on the entire pipeline of how societal risk materializes, rather than narrowly focusing on the role of AI and open foundation models. This expands the scope of possible interventions, such as protecting <a href="https://www.aisnakeoil.com/p/what-the-executive-order-means-for">downstream attack surfaces</a>. The framework is based on the <a href="https://owasp.org/www-community/Threat_Modeling">threat modeling framework</a> in cybersecurity and consists of six steps to analyze the risk of open foundation models:</p><ol><li><p><strong>Threat identification: </strong>Specify what the threat is and who it's from. For example, in cybersecurity, there are many potential risks from open foundation models, such as automated vulnerability detection and malware generation. Similarly, the marginal risk analysis would be very different if the risk is from individuals or small groups vs. heavily resourced groups such as state-sponsored attackers.</p></li><li><p><strong>Existing risk (absent open foundation models). </strong>In many cases, the risk of releasing models openly already exists in the world (though perhaps at a different level of severity). What is the existing risk of this threat?<strong>&nbsp;</strong></p></li><li><p><strong>Existing defenses (absent open foundation models). </strong>Similarly, many purported risks of open foundation models have existing defenses. For example, there are many existing defenses to the risk of phishing email scams. Email providers scan emails and senders' network traffic to prevent phishing and operating systems detect and warn users about malware downloaded from the internet.&nbsp;</p></li><li><p><strong>Marginal risk. </strong>Once the threat vector, existing risk level, and scope of existing defenses are clear, it is important to understand the marginal risk of releasing models openly&#8212;compared to existing technology such as the internet, as well as compared to releasing closed foundation models.&nbsp;</p></li><li><p><strong>Ease of defense against marginal risk. </strong>While existing defenses provide a baseline for addressing new risks introduced by open foundation models, new defenses can be implemented, or existing defenses can be modified to address the increase in overall risk. How easy is it to build or bolster defenses in response to the marginal risk?</p></li><li><p><strong>Uncertainty and assumptions. </strong>Finally, some disagreements might stem from unstated assumptions by different researchers about the open foundation model ecosystem. The framework asks researchers to specify the uncertainties and assumptions implicit in their analysis, to clarify disagreements between researchers with different viewpoints.</p></li></ol><p>In the paper, we give examples of how the framework can be used by looking at cybersecurity risks stemming from automated vulnerability detection and the risk of non-consensual deepfakes. For the former, we find that the current marginal risk of open foundation models is low and there are many approaches to defending against the marginal risk, including using AI for defense. For the latter, open foundation models pose considerable marginal risk at present, and plausible defenses seem hard. </p><p>The framework also helps clarify disagreements in past studies by revealing the different assumptions about risk. In fact, when we analyze studies across seven sources of risks against our framework, we find many of them lacking, and we hope the framework helps foster more constructive debate going forward. In particular, we encourage more grounded research on characterizing the marginal risk, especially as both <a href="https://arxiv.org/abs/2310.18233">model capabilities</a> and societal defenses evolve: evidence of minimal marginal risk today should not be seen as absolute evidence that risks may not arise in the future as underlying assumptions change.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a7df19-9bf6-42a1-9e18-3b44dd9d5c11_2952x934.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="461" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a7df19-9bf6-42a1-9e18-3b44dd9d5c11_2952x934.jpeg" width="1456" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Scoring studies that analyze the risk from open foundation models using our framework. &#11044; indicates the step of our framework is clearly addressed; &#9681; indicates partial completion; &#9711; indicates the step is absent in the misuse analysis. <a href="https://crfm.stanford.edu/open-fms/#risk-assessment">Full list of papers</a>.</figcaption></figure></div><p>We also analyze the benefits of open foundation models. We first look at key properties of openness:&nbsp; broader access (by allowing a wide range of people to access model weights), greater customizability (by allowing users to tune models to their needs), local adaptation and inference (users can run open models on the hardware of their choice) and an inability to rescind access (foundation model developers cannot revoke access easily once released).&nbsp;</p><p>These properties lead to many benefits (with some caveats):</p><ul><li><p><strong>Distributing who defines acceptable model behavior:</strong> Broader access to models and their greater customizability expands who is able to specify the boundary of acceptable model behavior, instead of this decision-making power lying solely with foundation model developers.</p></li><li><p><strong>Increasing innovation.</strong> Broader access, greater customizability, and local inference expand how foundation models are used to develop applications. For example, applications with strict privacy control requirements can use foundation models that run locally.</p></li><li><p><strong>Enabling scientific research.</strong> Many types of research on and using foundation models require access to model weights. In the last two years, we have already seen examples of increasing speed and outlining safety challenges enabled by open models. At the same time, access to other assets such as data, documentation, and model checkpoints is necessary for other kinds of research, so providing model weights alone is not a panacea.</p></li><li><p><strong>Enabling transparency.</strong> Broad access to weights enables some forms of transparency&#8212;for example, details about the model architecture. However, similar to research, transparency also requires assets other than model weights, notably public documentation, which is often lacking even when model weights are released openly.</p></li><li><p><strong>Mitigating monoculture and market concentration. </strong>The use of the same foundation model across different applications leads to monoculture. When the model fails, or something goes wrong, it then affects all of these downstream applications. Greater customizability mitigates some of the harms of monoculture since downstream users of foundation models can fine tune them to change their behavior. Similarly, broader access to models could help reduce market concentration in the downstream market by lowering the barrier to entry for developing different types of foundation models. At the same time, open foundation models are unlikely to reduce market concentration in the highly concentrated <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4543807">upstream markets</a> of computing and specialized hardware providers.</p></li></ul><h4><strong>Recommendations for developers, researchers, regulators, and policymakers</strong></h4><p>Our analysis of benefits and risks opens up concrete next steps for a wide range of stakeholders. In particular:</p><ul><li><p><strong>Developers of open foundation models</strong> should clarify the division of responsibility between them and the downstream users of the product. In particular, developers should clarify which responsible AI practices are implemented and which ones are left for downstream users who might modify the model for use in a consumer-facing application.</p></li><li><p><strong>Researchers investigating the risks of open foundation models</strong> should adopt the risk assessment framework to clearly articulate the marginal risk of releasing foundation models openly. Without such an assessment, it is unclear if the risks being outlined are also present in the status quo (i.e., without the release of open models) or if open models genuinely pose new risks that we cannot develop good defenses for.</p></li><li><p><strong>Policymakers</strong> should proactively assess the impact of proposed regulation on open foundation models, especially in the absence of authoritative evidence of their marginal risk. Funding agencies should ensure that research investigating the risks of open foundation models is sufficiently funded while remaining appropriately independent from the interests of foundation model developers.</p></li><li><p><strong>Competition regulators</strong> should invest in measuring the benefits of foundation models and the impact of openness on those benefits more systematically.</p></li></ul><p>We aim to rectify the conceptual confusion around the impact of openness by clearly identifying their distinctive properties and clarifying their benefits and risks. While some philosophical tensions surrounding the release strategies for open foundation models will probably never be resolved, we hope that our conceptual framework helps address today's deficits in empirical evidence.</p><h5><strong>Further reading</strong></h5><ul><li><p>The genesis of the paper was from a <a href="https://sites.google.com/view/open-foundation-models">workshop</a> we organized last September on responsible and open foundation models. Watch the video <a href="https://sites.google.com/view/open-foundation-models">here</a> and read a summary of the event <a href="https://hai.stanford.edu/news/how-promote-responsible-open-foundation-models">here</a>.</p></li><li><p>In December 2023, we wrote a <a href="https://hai.stanford.edu/issue-brief-considerations-governing-open-foundation-models">policy brief</a> on open foundation models based on lessons from our work, which is cited in NTIA's call for public input.</p></li><li><p>Our recent efforts are built on our prior work on <a href="https://crfm.stanford.edu/2022/05/17/community-norms.html">release norms</a> for foundation models, <a href="https://hai.stanford.edu/sites/default/files/2023-06/Response-to-Request.pdf">policy recommendations</a> for <a href="https://www.aisnakeoil.com/p/three-ideas-for-regulating-generative">AI regulation</a>, <a href="https://www.aisnakeoil.com/p/licensing-is-neither-feasible-nor">limitations</a> of <a href="https://hai.stanford.edu/sites/default/files/2023-11/AI-Regulatory-Alignment.pdf">licensing</a> AI development, and <a href="https://hai.stanford.edu/news/decoding-white-house-ai-executive-orders-achievements">analyses</a> of the Executive Order&#8217;s <a href="https://www.aisnakeoil.com/p/what-the-executive-order-means-for">impact on openness</a>.</p></li></ul>