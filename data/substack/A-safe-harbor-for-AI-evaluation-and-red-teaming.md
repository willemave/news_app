<p><em>This blog post is authored by Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Arvind Narayanan, Percy Liang, and Peter Henderson. The paper has 23 authors and is available <a href="https://sites.mit.edu/ai-safe-harbor/files/2024/03/Safe-Harbor-0e192065dccf6d83.pdf">here</a>.</em></p><p>Today, we are releasing an open letter encouraging AI companies to provide legal and technical protections for <a href="https://krebsonsecurity.com/2022/06/what-counts-as-good-faith-security-research/">good-faith</a> research on their AI models. The letter focuses on the importance of independent evaluations of proprietary generative AI models, particularly those with millions of users. In an accompanying paper, we discuss existing challenges to independent research and how a more equitable, transparent, and accountable researcher ecosystem could be developed.</p><p>The letter has been signed by hundreds of researchers, practitioners, and advocates across disciplines, and is open for signatures.&nbsp;</p><p><strong>Read and sign the open letter <a href="https://sites.mit.edu/ai-safe-harbor/">here</a>. Read the paper <a href="https://sites.mit.edu/ai-safe-harbor/files/2024/03/Safe-Harbor-0e192065dccf6d83.pdf">here</a>.</strong> </p><div class="subscription-widget-wrap-editor"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">You&#8217;re reading AI Snake Oil, a blog about our upcoming book. Subscribe to get new posts.</p></div><form class="subscription-widget-subscribe"><input class="email-input" name="email" tabindex="-1" type="email" /><input class="button primary" type="submit" value="Subscribe" /><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><h4><strong>Independent evaluation of AI is crucial for uncovering vulnerabilities</strong></h4><p><a href="https://openai.com/policies/sharing-publication-policy#research">AI</a> <a href="https://www.frontiermodelforum.org/">companies</a>, <a href="https://arxiv.org/pdf/2306.05949.pdf">academic</a> <a href="https://arxiv.org/pdf/2307.03718.pdf">researchers</a>, and <a href="https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendations-for-using-red-teaming-for-ai-accountability/">civil</a> <a href="https://epic.org/wp-content/uploads/2023/05/EPIC-Generative-AI-White-Paper-May2023.pdf">society</a> agree that generative AI models pose acute risks: independent risk assessment is an essential mechanism for providing accountability. Nevertheless, barriers exist that inhibit the independent evaluation of many AI models.</p><p>Independent researchers often evaluate and &#8220;red team&#8221; AI models to measure a variety of different risks. In this work, we focus on post-release evaluation of models (or APIs) by external researchers beyond the model developer. This is also referred to as <a href="https://www.ajl.org/bugs">algorithmic audits</a> by <a href="https://sustainabilitydigitalage.org/featured/wp-content/uploads/missing-links-in-ai-governance-unesco-mila.pdf#page=13">third parties</a>. Some companies also conduct red teaming before their models are released both internally and with experts they select.&nbsp;</p><p>While many types of testing are critical, independent evaluation of AI models that are already deployed is <a href="https://arxiv.org/abs/2307.03718">widely</a> <a href="https://www.ajl.org/bugs">regarded</a> as essential for ensuring safety, security, and trust. Independent red-teaming research of AI models has uncovered vulnerabilities related to <a href="https://arxiv.org/abs/2310.02446">low resource languages</a>, <a href="https://llm-tuning-safety.github.io/">bypassing</a> <a href="https://arxiv.org/abs/2310.02949">safety measure</a>, and a <a href="https://arxiv.org/abs/2307.02483">wide</a> <a href="http://www.jailbreakchat.com">range</a> of <a href="https://twitter.com/AIPanic">jailbreaks</a>. These evaluations investigate a broad set of often unanticipated model flaws, related to <a href="https://openai.com/research/practices-for-governing-agentic-ai-systems">misuse</a>, <a href="https://arxiv.org/pdf/2303.11408.pdf">bias</a>, <a href="https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem">copyright</a>, and other issues.</p><h4><strong>Terms of service can discourage community-led evaluations</strong></h4><p>Despite the need for independent evaluation, conducting research related to these vulnerabilities is often legally prohibited by the terms of service for popular AI models, including those of OpenAI, Google, Anthropic, Inflection, Meta, and Midjourney.</p><p>While these terms are intended as a deterrent against malicious actors, they also inadvertently restrict AI safety and trustworthiness research&#8212;companies forbid the research and may enforce their policies with account suspensions (as an example, see <a href="https://www.anthropic.com/legal/aup">Anthropic&#8217;s acceptable use policy</a>). While companies enforce these restrictions to varying degrees, the terms can disincentivize good-faith research by granting developers the right to terminate researchers' accounts or even take legal action against them. Often, there is limited transparency into the enforcement policy, and no formal mechanism for justification or appeal of account suspensions. Even aside from the legal deterrent, the risk of losing account access by itself may dissuade researchers who depend on these accounts for other critical types of AI research.&nbsp;</p><p>Evaluating the risks of models that are already deployed and have millions of users is essential as they pose immediate risks. However, only a relatively small group of researchers selected by companies have legal authorization to do so.&nbsp;</p><h4><strong>Existing safe harbors protect security research but not safety and trustworthiness research</strong></h4><p>AI developers have engaged to differing degrees with external red teamers and evaluators. For example, <a href="https://bugcrowd.com/openai">OpenAI</a>, <a href="https://security.googleblog.com/2023/10/googles-reward-criteria-for-reporting.html">Google</a>, and <a href="https://www.facebook.com/whitehat/info/">Meta</a> have bug bounties (that provide monetary rewards to people to report security vulnerabilities) and even legal protections for security research. Still, companies like <a href="https://www.facebook.com/whitehat/info/">Meta</a> and <a href="https://www.anthropic.com/responsible-disclosure-policy">Anthropic</a> currently &#8220;reserve final and sole discretion for whether you are acting in good faith and in accordance with this Policy,&#8221; which could deter good-faith security research. These legal protections extend only to traditional security issues like unauthorized account access and do not include broader safety and trustworthiness research.</p><p>Cohere and OpenAI are exceptions, though some ambiguity remains as to the scope of protected activities: Cohere <a href="https://docs.cohere.com/docs/usage-guidelines">allows</a> &#8220;intentional stress testing of the API and adversarial attacks&#8221; provided appropriate vulnerability disclosure (without explicit legal promises), and OpenAI expanded its safe harbor to include &#8220;model vulnerability research&#8221; and &#8220;academic model safety research&#8221; in response to an early draft of our proposal.</p><p>In the table below, we document gaps in the policies of leading AI companies. These gaps force well-intentioned researchers to either wait for approval from unresponsive researcher access programs, or risk violating company policy and losing access to their accounts.&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617f40f9-3f18-4766-9779-6dffa560b332_1600x900.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617f40f9-3f18-4766-9779-6dffa560b332_1600x900.jpeg" width="1456" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">The extent to which companies provide access to their flagship models, safe harbor for external security, safety, and trustworthiness research, as well as transparency and fairness in the enforcement of their policies. A transparent circle signifies that the company does not offer access to its model or safe harbor in that way, or that it is not transparent about how it enforces its policies. A half-filled circle indicates partial access, safe harbor, or transparency, and completely filled circles indicate substantial access, safe harbor, and transparency. See the paper for full details.</figcaption></figure></div><h4><strong>Our proposal: A legal and technical safe harbor</strong></h4><p>We believe that a pair of voluntary commitments could significantly improve participation, access, and incentives for public interest research into AI safety. The two commitments are a legal safe harbor, which protects good-faith, public-interest evaluation research provided it is conducted in accordance with well-established security vulnerability disclosure practices, and a technical safe harbor, which protects this evaluation research from account termination. Both safe harbors should be scoped to include research activities that uncover <em>any system flaws</em>, including all undesirable generations currently prohibited by a company&#8217;s terms of service.&nbsp;</p><p>As others have argued, this would not inhibit existing enforcement against malicious misuse, as protections are entirely contingent on abiding by the law and strict vulnerability disclosure policies, determined ex-post. The legal safe harbor, similar to a <a href="https://knightcolumbia.org/content/a-safe-harbor-for-platform-research">proposal</a> by the Knight First Amendment Institute for a safe harbor for research on social media platforms, would safeguard certain research from some amount of legal liability, mitigating the deterrent of strict terms of service. The technical safe harbor would limit the practical barriers to safety research from companies&#8217; enforcement of their terms by clarifying that researchers will not be penalized.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4b25d3a-536f-4215-98b1-763f73b1fa67_1097x764.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4b25d3a-536f-4215-98b1-763f73b1fa67_1097x764.png" width="1097" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">A summary of the proposed voluntary commitments from companies, and corresponding responsibilities for researchers required to enjoy the safe harbor protections. The company commitments are designed to protect <a href="https://krebsonsecurity.com/2022/06/what-counts-as-good-faith-security-research/">good faith</a> independent research on proprietary models, even when it exposes a company to criticism. The researcher commitments preserve privacy, prevents harms to users, or any disruption of business, among other concerns. Together these joint rules enable crowdsourced ethical hacking to improve public safety and awareness of problems.</figcaption></figure></div><h4><strong>A legal safe harbor reduces barriers to essential AI research</strong></h4><p>A legal safe harbor could provide assurances that AI companies will not sue researchers if their actions were taken for research purposes. In the US legal regime, this would impact companies&#8217; use of the Computer Fraud and Abuse Act (<a href="https://www.justice.gov/jm/jm-9-48000-computer-fraud">CFAA</a>) and Section 1201 of the Digital Millennium Copyright Act (<a href="https://www.copyright.gov/1201/2018/">DMCA</a>). These risks are not theoretical; security researchers have <a href="https://www.theregister.com/2001/04/23/sdmi_cracks_revealed/">been</a> <a href="https://www.wired.com/story/missouri-threatens-sue-reporter-state-website-security-flaw/">targeted</a> under the CFAA, and DMCA Section 1201 hampered security research to the extent that researchers <a href="https://github.blog/2021-11-23-copyright-office-expands-security-research-rights/">requested</a> and won an exemption from the law for this purpose. Already, in the context of generative AI, OpenAI has attempted to dismiss the <a href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html">NYTimes v OpenAI</a> lawsuit on the allegation that NYTimes research into the model <a href="https://www.reuters.com/technology/cybersecurity/openai-says-new-york-times-hacked-chatgpt-build-copyright-lawsuit-2024-02-27/">constituted hacking</a>.</p><p>These protections apply only to researchers who abide by companies&#8217; vulnerability disclosure policies, to the extent researchers can subsequently justify their actions in court. Research that is already illegal or does not take reasonable steps for responsible disclosure would fail in claiming those protections in an ex-post investigation. The Algorithmic Justice League has <a href="https://www.ajl.org/bugs">also</a> <a href="https://sustainabilitydigitalage.org/featured/wp-content/uploads/missing-links-in-ai-governance-unesco-mila.pdf#page=13">proposed</a> legal protections for third-party auditors of proprietary systems.</p><h4><strong>A technical safe harbor for AI safety and trustworthiness research removes practical deterrence</strong></h4><p>Legal safe harbors alone do not prevent account suspensions or other technical enforcement actions, such as rate limiting. These technical obstacles can also impede independent safety evaluations. We refer to the protection of research against these technical enforcement measures as a <em>technical</em> safe harbor. Without sufficient technical protections for public interest research, an asymmetry can develop between malicious and non-malicious actors, since non-malicious actors are discouraged from investigating vulnerabilities exploited by malicious actors.&nbsp;</p><p>We propose companies offer some path to eliminate these technical barriers for good-faith research, even when it can be critical of companies&#8217; models. This would include more equitable opportunities for researcher access and guarantees that those opportunities will not be foreclosed for researchers who adhere to companies' vulnerability disclosure policies. One way to do this is scale up researcher access programs and provide impartial review of applications for these programs. The challenge with implementing a technical safe harbor is distinguishing between legitimate research and malicious actors. An exemption to strict enforcement of companies&#8217; policies may need to be reviewed in advance, or at least when an unfair account suspension occurs. However, we believe this problem is tractable with participation from independent third parties.</p><h4><strong>Conclusion</strong></h4><p>The need for independent AI evaluation has garnered significant support from academics, journalists, and civil society. We identify legal and technical safe harbors as minimum fundamental protections for ensuring independent safety and trustworthiness research. They would significantly improve ecosystem norms, and drive more inclusive and unimpeded community efforts to tackle the risks of generative AI.</p><p><em>Cross-posted on the <a href="https://knightcolumbia.org/blog/a-safe-harbor-for-ai-evaluation-and-red-teaming">Knight First Amendment Institute</a> blog.</em></p><h4><strong>Further reading</strong></h4><p>Our work is inspired by and builds on several proposals in past literature:</p><p>Policy proposals directly related to protecting types of AI research from liability from the DMCA or CFAA:</p><ul><li><p>The Hacking Policy Council <a href="https://assets-global.website-files.com/62713397a014368302d4ddf5/6579fcd1b821fdc1e507a6d0_Hacking-Policy-Council-statement-on-AI-red-teaming-protections-20231212.pdf">proposes</a> that governments &#8220;clarify and extend legal protections for independent AI red teaming,&#8221; similar to our voluntary legal safe harbor proposal.</p></li><li><p>A <a href="https://www.copyright.gov/1201/2024/petitions/proposed/New-Pet-Jonathan-Weiss.pdf">petition</a> for a new exemption to the DMCA has been filed to facilitate research on AI bias and promote transparency in AI.</p></li></ul><p>Independent algorithmic audits and their design:</p><ul><li><p><a href="https://www.ajl.org/bugs">Bug Bounties for algorithmic harms? Lessons from cybersecurity vulnerability disclosure for algorithmic harms discovery, disclosure, and redress.</a></p></li><li><p><a href="https://sustainabilitydigitalage.org/featured/wp-content/uploads/missing-links-in-ai-governance-unesco-mila.pdf#page=13">Change from the Outside: Towards Credible Third-party Audits of AI Systems</a></p></li></ul><ul><li><p><a href="https://dl.acm.org/doi/10.1145/3531146.3533213">Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem</a></p></li></ul><p>Algorithmic bug bounties, safe harbors, and their design</p><ul><li><p><a href="https://www.usenix.org/conference/enigma2018/presentation/elazari">Are Bug Bounties a True Safe Harbor?</a></p></li><li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3290605.3300724">User Attitudes towards Algorithmic Opacity and Transparency in Online Reviewing Platforms</a></p></li><li><p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3161758">Private Ordering Shaping Cybersecurity Policy: The Case of Bug Bounties</a></p></li></ul><p>Other related proposals and red teaming work:</p><ul><li><p><a href="https://arxiv.org/abs/2401.14446">Black-Box Access is Insufficient for Rigorous AI Audits</a></p></li></ul><ul><li><p><a href="https://www.techpolicy.press/red-teaming-ai-the-devil-is-in-the-details/">Red Teaming AI: The Devil is in the Details</a></p></li><li><p><a href="https://crfm.stanford.edu/open-fms/paper.pdf">On the Societal Impact of Open Foundation Models</a></p></li><li><p><a href="https://hai.stanford.edu/news/time-now-develop-community-norms-release-foundation-models">Foundation Models Review Board</a></p></li><li><p><a href="https://cdn.governance.ai/Structured_Access_for_Third-Party_Research.pdf">Structured Access for Third-Party Research on Frontier AI Models: Investigating Researchers&#8217; Model Access Requirements</a></p></li><li><p><a href="https://arxiv.org/pdf/2307.03718.pdf">Frontier AI Regulation: Managing Emerging Risks to Public Safety</a></p></li><li><p><a href="https://aivillage.org/defcon%2031/generative-recap/">AI Village</a> regularly hosts events for large groups of independent researchers to red team models for non-security flaws, advancing collective knowledge of AI vulnerabilities.</p></li></ul>