<p>The assumption that AI safety is a property of AI models is pervasive in the AI community. It is seen as so obvious that it is hardly ever explicitly stated. Because of this assumption:</p><ul><li><p>Companies have made big investments in red teaming their models before releasing them.</p></li><li><p>Researchers are frantically trying to fix the <a href="https://www.aisnakeoil.com/p/model-alignment-protects-against">brittleness</a> of model alignment techniques.</p></li><li><p>Some AI safety advocates seek to restrict open models given concerns that they might pose unique risks.</p></li><li><p>Policymakers are trying to find the training compute threshold above which safety risks become serious enough to justify intervention (and lacking any meaningful basis for picking one, they seem to have converged on 10<sup>26</sup> rather arbitrarily).</p></li></ul><p>We think these efforts are inherently limited in their effectiveness. That&#8217;s because AI safety is not a model property. With a few exceptions, AI safety questions cannot be asked and answered at the levels of models alone. Safety depends to a large extent on the context and the environment in which the AI model or AI system is deployed. We have to specify a particular context before we can even meaningfully ask an AI safety question.</p><p>As a corollary, fixing AI safety at the model level alone is unlikely to be fruitful. Even if models themselves can somehow be made &#8220;safe&#8221;, they can easily be used for malicious purposes. That&#8217;s because an adversary can deploy a model without giving it access to the details of the context in which it is deployed. Therefore we cannot delegate safety questions to models &#8212; especially questions about misuse. The model will lack information that is necessary to make a correct decision.</p><p>Based on this perspective, we make four recommendations for safety and red teaming that would represent a major change to how things are done today.</p><p><strong>Safety depends on context: three examples</strong></p><p>Consider the concern that LLMs can help hackers generate and send phishing emails to a large number of potential victims. It&#8217;s true &#8212; in our own small-scale tests, we&#8217;ve <a href="https://knightcolumbia.org/content/how-to-prepare-for-the-deluge-of-generative-ai-on-social-media">found</a> that LLMs can generate persuasive phishing emails tailored to a particular individual based on publicly available information about them.&nbsp;</p><p>But here&#8217;s the problem: phishing emails are just regular emails! There is nothing intrinsically malicious about them. A phishing email might tell the recipient that there is an urgent deadline for a project they are working on, and that they need to click on a link or open an attachment to complete some action. What is malicious is the content of the webpage or the attachment. <em>But the model that&#8217;s being asked to generate the phishing email is not given access to the content that is potentially malicious</em>. So the only way to make a model refuse to generate phishing emails is to make it refuse to generate emails. That would affect many non-malicious uses, such as marketing.</p><p>We see the same pattern over and over. There has been alarm about LLMs being able to give bioterrorists information on how to create pathogens. But that information is <a href="https://www.rand.org/pubs/research_reports/RRA2977-2.html">readily available on the internet</a>. The hard parts for would-be bioterrorists are all of the <a href="https://www.nytimes.com/2024/03/08/technology/biologists-ai-agreement-bioweapons.html">other steps</a> involved: obtaining raw materials, culturing cells in the lab without killing them or infecting oneself, and disseminating the bioweapon to cause harm. AI could potentially aid that work, as it is a general-purpose tool and has some usefulness for almost all knowledge work. Again, this illustrates the limits of attempting to build safety into models: most of the questions the user would ask in this process relate to <em>synthetic biology in general and not bioweapons in particular</em>. To be sure that a model couldn&#8217;t assist bioterrorists, it would have to refuse to assist with any sort of bioengineering.</p><p>Or consider the use of LLMs to generate disinformation. Even in the unlikely event that a model could be aligned so that it refuses all requests to generate false information, research has found that true-but-misleading information is far more impactful than false information on social media; <a href="https://jenny-allen.com/publication/allen-2023-vaccine/">50x more</a> in the case of increasing vaccine hesitancy. So even a hypothetical safe model could be used to aid disinformation efforts: the adversary would use it to generate factual information (e.g. accurately summarizing news stories), with the misleading context added in separately.</p><p>In short, trying to make an AI model that can&#8217;t be misused is like trying to make a computer that can&#8217;t be used for bad things.</p><p><strong>Scope of our claims</strong></p><p>This essay is primarily about misuse, which seems to be the biggest driver of the AI safety worries recently. This includes both malicious misuse, such as the above examples, and nonmalicious misuse, such as students cheating on homework. Here again the model lacks the context to prevent only &#8220;bad&#8221; uses: it doesn&#8217;t know whether the task it is asked to perform is part of the user&#8217;s homework.</p><p>AI safety encompasses many other types of failures, such as bias and toxicity, accidents, reward hacking, and adversarial inputs (such as prompt injection). These are all different from misuse risks.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-1" id="footnote-anchor-1" target="_self">1</a> We think our argument applies in many of these cases, though less strongly. We don&#8217;t give a full analysis here. In the case of accidents, others have made the point that we have to look at the <a href="https://arxiv.org/pdf/2202.09292.pdf">system</a> and <a href="https://arxiv.org/pdf/2401.10899.pdf">context</a>, rather than the model alone.</p><p>Another related failure mode, one that is outside our scope, is overreliance on flawed models for legal or medical advice (whether this falls under AI safety is debatable but tangential to our point). To understand these harms, studying models makes sense: for example, a recent investigation by the <a href="https://www.proofnews.org/seeking-election-information-dont-trust-ai/">AI Democracy Projects</a> found that most models have high rates of incorrect responses to questions about the election.</p><p>Even within the category of misuse, there are a few exceptions to the rule that safety is not a model property. Some types of content are intrinsically problematic regardless of what someone does with it, as in the case of AI-child sexual abuse material.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-2" id="footnote-anchor-2" target="_self">2</a> Aligning AI systems to refuse such requests is important. Outputting memorized <a href="https://katelee168.github.io/pdfs/red-teaming-feb-2024.pdf">copyrighted material</a> is another such category.&nbsp;</p><p>In any case, our point is not that red teaming or aligning models is useless, just that safety has to be much broader than looking at models alone.</p><div class="subscription-widget-wrap-editor"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">You&#8217;re reading AI Snake Oil, a blog about our upcoming book. Subscribe to get new posts.</p></div><form class="subscription-widget-subscribe"><input class="email-input" name="email" tabindex="-1" type="email" /><input class="button primary" type="submit" value="Subscribe" /><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><p><strong>Recommendation 1: defenses against misuse must primarily be located outside models</strong></p><p>We&#8217;ve <a href="https://www.aisnakeoil.com/p/model-alignment-protects-against">written before</a> that model alignment can easily be evaded by adversaries. Those evasive techniques, such as jailbreaks, are potentially fixable. Here, we are talking about something more fundamental: misuse that does not require breaching the alignment guarantees in any way, such as writing persuasive emails that can be used for either marketing or phishing.</p><p>If model alignment is not the answer, other defenses are sorely needed. As we&#8217;ve consistently <a href="https://www.aisnakeoil.com/p/three-ideas-for-regulating-generative">argued</a>, defenses should focus on attack surfaces: the downstream sites where attackers use the outputs of AI models for malicious purposes. For example, the best defenses against phishing emails, whether generated by humans or LLMs, are email scanners and URL blacklists &#8212; which we&#8217;ve had for a couple of decades and have gradually gotten pretty good, although of course we must continue to improve them.</p><p>If we instead keep barking up the tree of model alignment, the fact that the model lacks access to context, and therefore can&#8217;t make informed safety determinations, will lead to both false positives and false negatives. In other words, it will not only lead to a failure to prevent misuse, but also the opposite problem: refusing innocuous requests like an overzealous censor.</p><p><strong>Recommendation 2: assess marginal risk</strong></p><p>If safety is not primarily a model property and defenses must reside elsewhere, then there might not be a big difference between the safety implications of open and closed release strategies. In any case, the debate on openness in AI needs a more rigorous risk assessment framework. We were recently part of a large collaboration that presented just such a <a href="https://crfm.stanford.edu/open-fms/">framework</a>. It enables assessing the <em>marginal</em> risk of releasing a model &#8212; that is, the additional or incremental risk &#8212; compared to the risk from existing models (and non-AI technologies). It takes into account that defenses for some risks might already exist, especially defenses located outside models. Using this framework, we showed that the marginal risk of open models in cybersecurity (specifically, enabling automated vulnerability detection) is low, whereas for the generation of non-consensual intimate imagery, the marginal risk is substantial.</p><p>A notable potential safety advantage of closed models is the ability to monitor queries and <em>retrospectively</em> identify malicious use. This is a far easier technical problem than building safety into the model itself.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-3" id="footnote-anchor-3" target="_self">3</a> Besides, the risk of account suspension or prosecution might exert a deterrent effect on threat actors. In any case, this sort of comparison between open and closed models will have to be made separately for each type of misuse based on empirical evidence. Currently, we don&#8217;t have reliable evidence of how well monitoring and detection is working because of the lack of transparency by developers. One small but notable exception is a recent <a href="https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/">blog post</a> on Microsoft&#8217;s and OpenAI&#8217;s efforts to detect and disrupt hacking groups&#8217; use of LLMs.</p><p><strong>Recommendation 3: refocus red teaming toward early warning</strong></p><p>We should not expect red teaming to tell us whether or not a model can be misused (the answer is always yes). Instead, we should use red teaming to learn about the advancing frontier of adversary capabilities enabled by state-of-the-art AI models. For example, if AI systems have gotten powerful enough to automate a complex cybersecurity attack chain &#8212; scanning social media profiles to gather information, crafting a phishing email, taking over an account, exfiltrating information, and concealing traces of the attack &#8212; we need to have early warning of those capabilities so that we can defend appropriately.</p><p>To do this, we may need to design better offensive pipelines, such as for hacking. In the case of disinformation, a key offensive capability would be building a bot that can engage in a persuasive conversation on political topics over a long period of time. Building such capabilities raises ethical challenges. The cybersecurity community has long grappled with these challenges, and the <a href="https://www.schneier.com/essays/archives/2008/05/the_ethics_of_vulner.html">general conclusion</a> is that we are better off in a world where everyone has access to offensive capabilities than one in which only attackers do.</p><p>The results of red teaming should inform the development of defenses &#8212; defenses that almost always will reside <em>outside</em> AI models (such as detecting and labeling bot accounts on social media).</p><p><strong>Recommendation 4: red teaming should be led by third parties with aligned incentives</strong></p><p>The above change in objectives of red teaming leads to a subtle shift in incentives. When red teaming is model focused, developers have an incentive to do a good job. If they find that models produce &#8220;dangerous&#8221; information, they can fix that behavior, which helps them avoid bad press.</p><p>But for the kind of misuse we&#8217;re talking about, the incentives are reversed. It is not in developers&#8217; interest to build the most powerful offensive pipeline possible. If they do, they might find that (for instance) a model can be used for hacking, but they will have no way to prevent this. Thus, they will have to admit that they are knowingly releasing a model that can be used for offensive purposes. It is much better for them to not find out in the first place.&nbsp;</p><p>Consider OpenAI's <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation">recent study</a> on biological threats from language models. OpenAI evaluated the risk of users gaining access to information using language models compared to the internet (which is much better than previous studies that don't use the internet as a baseline at all). But creating a bioweapon requires far more than a few hours of information hunting. A motivated actor needs access to a lab, reagents, and equipment in order to even begin the process. The real question is whether AI can help adversaries acquire these resources. The study does not answer that.</p><p>The incentives of third parties are potentially better aligned for a more holistic risk assessment that is less focused on models alone. But here too, there is need for caution. Until recently, much of the evidence for biosecurity risks of language models came from groups funded by a small number of <a href="https://1a3orn.com/sub/essays-propaganda-or-science.html">effective altruism organizations</a>. Members of the U.S. House Committee on Science, Space, and Technology recently wrote a <a href="https://republicans-science.house.gov/_cache/files/8/a/8a9f893d-858a-419f-9904-52163f22be71/191E586AF744B32E6831A248CD7F4D41.2023-12-14-aisi-scientific-merit-final-signed.pdf">letter</a> expressing concern about the lack of transparency in how the National Institute of Standards and Technology and the AI Safety Institute plan to allocate funding for third-party evaluations. They were especially concerned about upholding the standards of scientific research.</p><p><strong>Final thoughts: developer responsibility</strong></p><p>Why has the myth of safety as a model property persisted? Because it would be convenient for everyone if it were true! In a world where safety is a model property, companies could confidently determine whether a model is safe enough to release, and AI researchers could apply their arsenal of technical methods toward safety. Most importantly, accountability questions would have relatively clear answers. Companies should have liability for harms if model safety guarantees fail, but not otherwise.</p><p>By contrast, accepting that there is no technical fix to misuse risks means that the question of responsibility is extremely messy, and we don&#8217;t currently have a good understanding of how to allocate liability for misuse. Assuming that retrospective detection is easier (see recommendation 2 above), one low-hanging fruit is to require anyone who hosts a model, whether closed or open, to adhere to certain standards for monitoring and reporting misuse &#8212; see our call for generative AI companies to publish <a href="https://www.aisnakeoil.com/p/generative-ai-companies-must-publish">transparency reports</a> (and, more generally, the <a href="https://www.lawfaremedia.org/article/cybersecurity-and-least-cost-avoide">least cost avoider</a> principle). But that won&#8217;t be enough, and downstream defenses are needed.</p><p>Unfortunately, downstream defenses against misuse impose a great cost on the rest of society. For example, the fact that any image or video could be AI-generated means that realism is no longer a marker of authenticity. That means we all need to adapt and change how we assess the veracity of information online. And that&#8217;s just one of dozens of such adaptations needed.</p><p>Morally speaking, developers should bear some of the societal costs of harmful uses of AI, mirroring the fact that they reap profits from beneficial uses of AI. But legally speaking, we have no tool to enforce that. Remedying this situation is the great challenge of AI policy &#8212; a point we&#8217;ve made <a href="https://www.aisnakeoil.com/p/generative-ais-end-run-around-copyright">over</a> and <a href="https://knightcolumbia.org/content/how-to-prepare-for-the-deluge-of-generative-ai-on-social-media">over</a>. No amount of &#8220;guardrails&#8221; will close this gap.</p><p><strong>Acknowledgment.</strong> Arvind Narayanan is grateful for being invited to a National AI Advisory Committee AI Safety <a href="https://www.nist.gov/system/files/documents/2024/03/01/Agenda%20NAIAC%203.5.24-for%20web.pdf">panel</a>, where he presented some of these ideas. We thank Mihir Kshirsagar for feedback on a draft. </p><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-1" id="footnote-1" target="_self">1</a><div class="footnote-content"><p>Prompt injection might enable misuse, but this post is about misuse that doesn&#8217;t require violating model safety properties.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-2" id="footnote-2" target="_self">2</a><div class="footnote-content"><p>Update: we originally used nonconsensual intimate imagery (NCII) as an example here. But NCII is not an exception to the rule; in fact, it is an excellent example of it. The model does not know whether the request is consensual or not; so the only way to refuse to generate NCII is to refuse to generate any intimate imagery. We are grateful to <a href="https://twitter.com/belladoreai/status/1767899529251111188">@belladoreai</a> on Twitter/X for pointing this out. That said, blanket refusal to generate nudes is a worthwhile tradeoff that many model developers make, unlike refusing all requests to generate emails, which is hard to justify. And if they've made that tradeoff, they can try to enforce it at the model level (at least for closed models). In short, NCII is not an exception to the pattern, but it does have a different relative cost of false positives and false negatives.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-3" id="footnote-3" target="_self">3</a><div class="footnote-content"><p>There are many reasons why forensics is easier than model alignment. It only requires assessing whether an <em>account</em>,<em> </em>which may have made hundreds of queries, has violated usage policies, rather than making assessments query by query, so there is a lot more information to go on. Analysts might also be able to use a materialized attack as evidence of intent: for example, a disinformation campaign on social media that used content generated by a model. Broadly speaking, real-time AI assessment is no match for patient human analysis in making nuanced determinations of malicious use.</p></div></div>