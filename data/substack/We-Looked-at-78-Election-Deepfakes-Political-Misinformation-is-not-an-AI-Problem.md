<p>AI-generated misinformation was one of the top concerns during the 2024 U.S. presidential election. In January 2024, the World Economic Forum <a href="https://www.weforum.org/stories/2024/01/ai-disinformation-global-risks/">claimed</a> that &#8220;misinformation and disinformation is the most severe short-term risk the world faces&#8221; and that &#8220;AI is amplifying manipulated and distorted information that could destabilize societies.&#8221; News headlines about elections in 2024 tell a similar story:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29800e81-2362-430e-ada1-591a2d3a5228_910x1000.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="507.6923076923077" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29800e81-2362-430e-ada1-591a2d3a5228_910x1000.jpeg" width="462" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a></figure></div><p>In contrast, in our past writing, we predicted that AI would not lead to a misinformation apocalypse. When Meta released its open-weight large language model (called LLaMA), we <a href="https://www.aisnakeoil.com/p/the-llama-is-out-of-the-bag-should">argued</a> that it would not lead to a tidal wave of misinformation. And in a follow-up essay, we <a href="https://knightcolumbia.org/content/how-to-prepare-for-the-deluge-of-generative-ai-on-social-media">pointed out</a> that the distribution<em> </em>of misinformation is the key bottleneck for influence operations, and while generative AI reduces the cost of creating misinformation, it does not reduce the cost of distributing it. A few other researchers have made <a href="https://misinforeview.hks.harvard.edu/article/misinformation-reloaded-fears-about-the-impact-of-generative-ai-on-misinformation-are-overblown/">similar arguments</a>.</p><p>Which of these two perspectives better fits the facts?</p><p>Fortunately, we have the evidence of AI use in elections that took place around the globe in 2024 to help answer this question. Many news outlets and research projects have compiled known instances of AI-generated text and media and their impact. Instead of speculating about AI&#8217;s potential, we can look at its real-world impact to date.</p><p>We analyzed every instance of AI use in elections collected by the <a href="https://www.wired.com/story/generative-ai-global-elections/">WIRED AI Elections Project</a>, which tracked known uses of AI for creating political content during elections taking place in 2024 worldwide. In each case, we identified what AI was used for and estimated the cost of creating similar content without AI.</p><p>We find that (1) half of AI use isn't deceptive, (2) deceptive content produced using AI is nevertheless cheap to replicate <em>without </em>AI, and (3) focusing on the demand for misinformation rather than the supply is a much more effective way to diagnose problems and identify interventions.</p><p>To be clear, AI-generated synthetic content poses many real dangers: the creation of <a href="https://www.pbs.org/newshour/show/nonconsensual-sexual-images-posted-online-made-worse-by-deepfakes-and-ai-technology">non-consensual images of people</a> and <a href="https://www.washingtonpost.com/technology/2024/04/22/ai-csam-ncmec-cybertipline-stanford-report/">child sexual abuse material</a> and the enabling of the <a href="https://www.brookings.edu/articles/misunderstood-mechanics-how-ai-tiktok-and-the-liars-dividend-might-affect-the-2024-elections/">liar&#8217;s dividend</a>, which allows those in power to brush away real but embarrassing or controversial media content about them as AI-generated. These are all important challenges. This essay is focused on a different problem: political misinformation.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-1" id="footnote-anchor-1" target="_self">1</a></p><p>Improving the information environment is a difficult and ongoing challenge. It&#8217;s understandable why people might think AI is making the problem worse: AI does make it possible to fabricate false content. But that has not fundamentally changed the landscape of political misinformation.</p><p>Paradoxically, the alarm about AI might be comforting because it positions concerns about the information environment as a discrete problem with a discrete solution. But fixes to the information environment depend on structural and institutional changes rather than on curbing AI-generated content.</p><h3><strong>Half of the Deepfakes in 2024 Elections weren&#8217;t Deceptive</strong></h3><p>We analyzed all 78 instances of AI use in the WIRED AI Elections Project (<a href="https://www.cs.princeton.edu/~sayashk/political-misinformation/WIRED-data.html">source</a> for our analysis).<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-2" id="footnote-anchor-2" target="_self">2</a> We categorized each instance based on whether there was deceptive intent. For example, if AI was used to generate false media depicting a political candidate <a href="https://www.npr.org/2024/01/22/1226129926/nh-primary-biden-ai-robocall">saying something they didn't</a>, we classified it as deceptive. On the other hand, if a chatbot gave an <a href="https://www.proofnews.org/ai-models-falter-answering-election-questions-in-spanish/">incorrect response</a> to a genuine user query, a deepfake was created for <a href="https://restofworld.org/2024/exporter-india-deepfake-trolls/">parody or satire</a>, or a candidate transparently used AI to improve their campaigning materials (such as by <a href="https://www.hindustantimes.com/technology/pm-modi-uses-ai-tool-bhashini-at-kashi-tamil-sangamam-in-varanasi-what-is-it-101702837254702.html">translating</a> a speech into a language they don't speak), we classify it as non-deceptive.</p><p>To our surprise, there was no deceptive intent in 39 of the 78 cases in the database.</p><p>The most common non-deceptive use of AI was for campaigning. When candidates or supporters used AI for campaigning, in most cases (19 out of 22), the apparent intent was to improve campaigning materials rather than mislead voters with false information.</p><p>We even found examples of deepfakes that we think helped improve the information environment. In Venezuela, journalists used AI avatars to <a href="https://globalvoices.org/2024/08/19/venezuelans-use-ai-avatars-and-instagram-live-to-fight-back-maduros-repression/">avoid</a> government retribution when covering news adversarial to the government. In the U.S., a local news organization from Arizona, Arizona Agenda, used deepfakes to <a href="https://www.washingtonpost.com/politics/2024/03/24/kari-lake-deepfake/">educate</a> viewers about how easy it is to manipulate videos. In California, a candidate with laryngitis lost his voice, so he transparently used AI <a href="https://www.youtube.com/watch?v=LSlXWjMKL5E">voice cloning</a> to read out typed messages in his voice during meet-and-greets with voters.</p><p>Reasonable people can disagree on whether using AI in campaigning materials is legitimate or what the appropriate guardrails need to be. But using AI for campaign materials in non-deceptive ways (for example, when AI is used as a tool to improve voter outreach) is much less problematic than deploying AI-generated fake news to sway voters.</p><p>Of course, not all non-deceptive AI-generated political content is benign.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-3" id="footnote-anchor-3" target="_self">3</a> Chatbots often <a href="https://www.proofnews.org/seeking-election-information-dont-trust-ai/">incorrectly</a> answer election-related questions. Rather than deceptive intent, this results from the limitations of chatbots, such as hallucinations and lack of factuality. Unfortunately, these limitations are not made clear to users, leading to an overreliance on flawed large language models (LLMs).<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-4" id="footnote-anchor-4" target="_self">4</a></p><h3><strong>Making Deceptive Political Misinformation Does Not Require AI</strong></h3><p>For each of the 39 examples of deceptive intent, where AI use was intended to make viewers believe outright false information, we estimated the cost of creating similar content <em>without</em> AI&#8212;for example, by hiring Photoshop experts, video editors, or voice actors. In each case, the cost of creating similar content without AI was modest&#8212;no more than a few hundred dollars. (We even found that a video involving a <a href="https://www.latimes.com/california/story/2024-09-24/fake-russian-news-site-falsely-claimed-kamala-harris-was-in-hit-and-run-accident">hired stage actor</a> was incorrectly marked as being AI-generated in WIRED&#8217;s election database.)</p><p>In fact, it has long been possible to create media with outright false information without using AI or other fancy tools. One video used <a href="https://www.latimes.com/california/story/2024-09-24/fake-russian-news-site-falsely-claimed-kamala-harris-was-in-hit-and-run-accident">stage actors</a> to falsely claim that U.S. Vice President and Democratic presidential candidate Kamala Harris was involved in a hit-and-run incident. Another <a href="https://www.usatoday.com/story/news/factcheck/2024/09/30/kamala-harris-slur-speech-helene-fact-check/75447979007/">slowed down</a> the vice president's speech to make it sound like she was slurring her words. An edited video of Indian opposition candidate Rahul Gandhi showed him <a href="https://archive.is/SxXeX#selection-1109.0-1126.0">saying</a> that the incumbent Narendra Modi would win the election. In the original video, Gandhi said his opponent would <em>not </em>win the election, but it was <a href="https://www.boomlive.in/fact-check/factcheck-rahul-gandhi-modi-will-become-pm-again-doctored-video-fake-news-25264">edited</a> using jump cuts to take out the word &#8220;not.&#8221; Such media content has been called &#8220;<a href="https://datasociety.net/wp-content/uploads/2019/09/DataSociety_Deepfakes_Cheap_Fakes.pdf">cheap fakes</a>&#8221; (as opposed to AI-generated &#8220;deepfakes&#8221;).</p><p>There were many instances of cheap fakes used in the 2024 U.S. election. The News Literacy Project <a href="https://misinfodashboard.newslit.org/">documented</a> known misinformation about the election and <a href="https://newslit.org/newsroom/press-release/the-news-literacy-project-experts-watch-out-for-ai-generated-fakes-and-disinformation-about-voting-ahead-of-election-day/">found</a> that cheap fakes were used seven times more often than AI-generated content. Similarly, in other countries, cheap fakes were quite prevalent. An <a href="https://www.boomlive.in/">India-based fact checker</a> reviewed an order of magnitude more cheap fakes and traditionally edited media compared to <a href="https://www.boomlive.in/deepfake-tracker">deepfakes</a>. In Bangladesh, cheap fakes were over <a href="https://www.context.news/ai/opinion/cheap-fakes-are-a-blind-spot-for-platforms-in-the-global-south">20 times more prevalent</a> than deepfakes.</p><p>Let&#8217;s consider two examples to analyze how cheap fakes could have led to substantially similar effects as the deepfakes that got a lot of media attention: Donald Trump&#8217;s use of Taylor Swift deepfakes to campaign and a voice-cloned robocall that imitated U.S. President Joe Biden in the New Hampshire primary asking voters not to vote.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faab44993-d832-4881-9532-f78e94883231_1222x1466.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="532.6546644844517" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faab44993-d832-4881-9532-f78e94883231_1222x1466.jpeg" width="444" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>A Truth Social post shared by Donald Trump with images of Taylor Swift fans wearing &#8220;Swifties for Trump&#8221; t-shirts. Top left: A post with many AI-generated images of women wearing &#8220;Swifties for Trump&#8221; t-shirts, with a &#8220;satire&#8221; label. Top right: A <a href="https://www.cbsnews.com/news/trump-shares-fake-swifties-for-trump-images/">real image</a> of Trump supporter Jenna Piwowarczyk wearing a &#8220;Swifties for Trump&#8221; t-shirt. Bottom left: A fabricated image of Taylor Swift in front of the American flag with the caption, &#8220;Taylor wants you to vote for Donald Trump.&#8221; It is unclear if the image was created using AI or other editing software. Bottom right: A Twitter post with two images: one AI-generated, the other real, of women wearing &#8220;Swifties for Trump&#8221; t-shirts.</em></figcaption></figure></div><p>Trump&#8217;s use of Swift deepfakes implied that Taylor Swift had endorsed him and that Swift fans were attending his rallies en masse. In the wake of the post, many <a href="https://www.forbes.com/sites/steveandriole/2024/08/21/taylor-swift-donald-trump-and-ai/">media</a> <a href="https://www.lowyinstitute.org/the-interpreter/what-taylor-swift-taught-world-risks-ai-generated-images-elections">outlets</a> <a href="https://www.latimes.com/entertainment-arts/business/story/2024-08-21/deep-fakes-social-media-taylor-swift-donald-trump">blamed</a> AI for the spread of misinformation.</p><p>But recreating similar images without AI is easy. Images depicting Swift&#8217;s support could be created by photoshopping text endorsing Trump onto any of her existing images. Likewise, getting images of Trump supporters wearing &#8220;Swifties for Trump&#8221; t-shirts could be achieved by distributing free t-shirts at a rally&#8212;or even selectively reaching out to Swift fans at Trump rallies. In fact, two of the images Trump shared were real images of a Trump <a href="https://www.cbsnews.com/news/trump-shares-fake-swifties-for-trump-images/">supporter who is also a Swift fan</a>.</p><p>Another incident that led to a brief panic was an AI clone of President Joe Biden&#8217;s voice that <a href="https://www.npr.org/2024/01/22/1226129926/nh-primary-biden-ai-robocall">asked people not to vote</a> in the New Hampshire primary.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a8a7210-696d-457a-8997-8be9775e5d92_1022x1296.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="598.5440313111546" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a8a7210-696d-457a-8997-8be9775e5d92_1022x1296.jpeg" width="472" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em>News headlines in the wake of the Biden robocall.</em></figcaption></figure></div><p>Rules against such robocalls have existed for years. In fact, the perpetrator of this particular robocall was <a href="https://www.reuters.com/world/us/fcc-finalizes-6-million-fine-over-ai-generated-biden-robocalls-2024-09-26/">fined $6 million</a> by the Federal Communications Commission (FCC). The FCC has tiplines to report similar attacks, and it <a href="https://fccprod.servicenowservices.com/rmd?id=rmd_listings">enforces rules around robocalls frequently</a>, regardless of whether AI is used. Since the robocall used a static recording, it could have been made about as easily without using AI&#8212;for instance, by hiring voice impersonators.</p><p>It is also unclear what impact the robocall had: The efficacy of the deepfake depends on the recipient believing that the <em>president of the United States</em> is personally calling them on the phone to ask them <em>not </em>to vote in a primary.</p><p>Is it just a matter of time until improvements in technology and the expertise of actors seeking to influence elections lead to more effective AI disinformation? We don&#8217;t think so. In the next section, we point out that structural reasons that drive the demand for misinformation are not aided by AI. We then look at the history of predictions about coming waves of AI disinformation that have accompanied the release of new tools&#8212;predictions that have not come to pass.</p><h3><strong>The Demand for Misinformation</strong></h3><p>Misinformation can be seen through the forces of <a href="https://reason.com/volokh/2023/10/05/the-demand-for-political-misinformation-is-a-bigger-problem-than-the-supply-even-in-the-age-of-ai/">supply and demand</a>. The supply comes from people who want to make a buck by generating clicks, partisans who want their side to win, or state actors who want to conduct influence operations. Interventions so far have almost entirely tried to curb the supply of misinformation while leaving the demand unchanged.</p><p>The focus on AI is the latest example of this trend. Since AI reduces the cost of generating misinformation to nearly zero, analysts who look at misinformation as a supply problem are very concerned. But analyzing the <em>demand </em>for misinformation can clarify how misinformation spreads and what interventions are likely to help.</p><p>Looking at the demand for misinformation tells us that as long as people have certain worldviews, they will seek out and find <a href="https://dl.acm.org/doi/10.1145/2470654.2481326">information consistent with those views</a>. Depending on what someone&#8217;s worldview is, the information in question is often misinformation&#8212;or at least would be considered misinformation by those with differing worldviews.</p><p>In other words, successful misinformation operations <a href="https://dl.acm.org/doi/10.1145/3091478.3091523">target in-group members</a>&#8212;people who <a href="https://misinforeview.hks.harvard.edu/wp-content/uploads/2024/04/traberg_gamified_inoculation_political_ingroups_20240430.pdf">already</a> <a href="https://www.nber.org/papers/w28884">agree</a> with the broad intent of the message. Such recipients may have lower skepticism for messages that conform to their worldviews and may even be willing to knowingly amplify <a href="https://ohiocapitaljournal.com/2024/09/16/voters-moral-flexibility-helps-them-defend-politicians-misinformation/">false information</a>. Sophisticated tools aren&#8217;t needed for misinformation to be effective in this context. On the flip side, it will be extremely hard to convince <a href="https://www.science.org/doi/10.1126/sciadv.abf1234?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed#sec-2">out-group members</a> of false information that they <em>don't </em>agree with, regardless of AI use.</p><p>Seen in this light, AI misinformation plays a very different role from its popular depiction of swaying voters in elections. Increasing the supply of misinformation does not meaningfully change the dynamics of the <em>demand</em> for misinformation since the increased supply is <a href="https://www.nature.com/articles/s41562-020-0833-x">competing for the same eyeballs.</a><strong> </strong>Moreover, the increased supply of misinformation is <a href="https://cetas.turing.ac.uk/publications/ai-enabled-influence-operations-safeguarding-future-elections">likely to be consumed</a> mainly by a <a href="https://www.nature.com/articles/s41562-023-01564-2.pdf">small</a> <a href="https://www.science.org/doi/10.1126/science.aau2706">group</a> of partisans who <a href="https://www.nature.com/articles/s41598-022-19837-7.pdf">already agree with it</a> and heavily consume misinformation rather than to convince a broader swath of the public.</p><p>This also explains why <a href="https://datasociety.net/wp-content/uploads/2019/09/DataSociety_Deepfakes_Cheap_Fakes.pdf">cheap fakes</a> such as media from unrelated events, traditional video edits such as jump cuts, or even <a href="https://www.wired.com/story/x-israel-hamas-war-disinformation/">video game footage</a> can be effective for propagating misinformation despite their low quality: It is much easier to convince someone of misinformation if they already agree with its message.</p><p>Our analysis of the demand for misinformation may be most applicable to countries with polarized close races where leading parties have similar capacities for voter outreach, so that voters&#8217; (mis)information demands are already saturated.</p><p>Still, to our knowledge, in every country that held elections in 2024 so far, AI misinformation had much <a href="https://www.context.news/ai/opinion/cheap-fakes-are-a-blind-spot-for-platforms-in-the-global-south">less impact than feared</a>. In India, deepfakes were used for <a href="https://restofworld.org/2024/exporter-india-deepfake-trolls/">trolling</a> more than spreading false information. In Indonesia, the impact of AI wasn't to sow false information but rather to soften the image of then-candidate, now-President Prabowo Subianto (a former general accused of many past human rights abuses) using <a href="https://www.cigionline.org/articles/its-time-to-reframe-disinformation-indonesias-elections-show-why/">AI-generated digital cartoon avatars</a> that depicted him as likable.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-5" id="footnote-anchor-5" target="_self">5</a></p><h3><strong>Why Do Concerns About AI Misinformation Keep Recurring?</strong></h3><p>The 2024 election cycle wasn&#8217;t the first time when there was widespread fear that AI deepfakes would lead to rampant political misinformation. <a href="https://www.cnn.com/2019/06/12/tech/deepfake-2020-detection/index.html">Strikingly similar concerns</a> about AI were expressed before the 2020 U.S. election, though these concerns <a href="https://www.npr.org/2020/10/01/918223033/where-are-the-deepfakes-in-this-presidential-election">were not borne out</a>. The release of new AI tools is often accompanied by worries that it will unleash new waves of misinformation:</p><ul><li><p><strong>2019. </strong>When OpenAI released its GPT-2 series of models in 2019, one of the main reasons it <a href="https://www.technologyreview.com/2019/08/29/133218/openai-released-its-fake-news-ai-gpt-2/">held back</a> on releasing the model weights for the most capable models in the series was its alleged potential to generate misinformation.</p></li><li><p><strong>2023. </strong>When Meta released the LLaMA model openly in 2023, multiple news outlets reported <a href="https://www.nytimes.com/2023/05/18/technology/ai-meta-open-source.html">concerns</a> that it would trigger a deluge of AI misinformation. These models were far more powerful than the GPT-2 models released by OpenAI in 2019. Yet, we have not seen evidence of large-scale voter persuasion attributed to using LLaMA or other large language models.</p></li><li><p><strong>2024. </strong>Most recently, the widespread availability of AI image editing tools on smartphones has prompted similar <a href="https://www.theverge.com/2024/8/26/24228808/ai-image-editing-photoshop-comparison-argument">concerns</a>.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97a992ce-4187-46e1-b7cd-85a357716c64_779x855.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="535.609756097561" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97a992ce-4187-46e1-b7cd-85a357716c64_779x855.png" width="488" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption"><em><a href="https://newsletter.pessimistsarchive.org/p/the-1912-war-on-fake-photos">Source</a>, <a href="https://www.newspapers.com/article/janesville-weekly-gazette/153866353/">Primary source</a></em></figcaption></figure></div><p>In fact, concerns about using new technology to create false information go back over a century. The late 19th and early 20th centuries saw the advent of technologies for photo retouching. This was accompanied by concerns that retouched photographs would be used to deceive people, and, in 1912, a bill was introduced in the U.S. that would have criminalized <a href="https://newsletter.pessimistsarchive.org/p/the-1912-war-on-fake-photos">photo editing</a> without subjects&#8217; consent. (It died in the Senate.)</p><p>Thinking of political misinformation as a <a href="https://x.com/JMchangama/status/1857136505472405680">technological (or AI)</a> problem is <a href="https://www.technologyreview.com/2024/09/03/1103464/ai-impact-elections-overblown/">appealing</a> because it makes the solution seem tractable. If only we could roll back harmful tech, we could drastically improve the information environment!</p><p>While the goal of improving the information environment is laudable, blaming technology is not a fix. Political polarization has <a href="https://www.taylorfrancis.com/chapters/edit/10.4324/9780203713020-4/distrust-news-media-symptom-cause-partisan-polarization-jonathan-ladd-alexander-podkul">led</a> to greater <a href="https://shorensteincenter.org/future-trustworthy-information-learning-online-content-creators/">mistrust</a> of the media. People prefer sources that <a href="https://dl.acm.org/doi/10.1145/2470654.2481326">confirm</a> their worldview and are less <a href="https://academic.oup.com/restud/advance-article-abstract/doi/10.1093/restud/rdae058/7685990?redirectedFrom=fulltext">skeptical</a> about content that <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/1121D46E918815B2CD7A9C6C237464A2/S1930297500003570a.pdf/susceptibility-to-misinformation-is-consistent-across-questionframings-and-response-modes-and-better-explained-by-myside-bias-and-partisanshipthan-analytical-thinking.pdf#page=20.60">fits</a> their worldview. Another major factor is the <a href="https://www.census.gov/library/stories/2022/06/internet-crushes-traditional-media.html">drastic decline</a> of <a href="https://www.pewresearch.org/journalism/fact-sheet/newspapers/">journalism revenues</a> in the last two decades&#8212;largely driven by the shift from traditional to social media and online advertising. But this is more a result of structural changes in how people seek out and consume information than the specific threat of misinformation shared online.</p><p>As history professor Sam Lebovic has pointed out, improving the information environment is <a href="https://knightcolumbia.org/content/fake-news-lies-and-other-familiar-problems">inextricably linked</a> to the larger project of shoring up democracy and its institutions. There&#8217;s no quick technical fix, or targeted regulation, that can &#8220;solve&#8221; our information problems. We should reject the simplistic temptation to blame AI for political misinformation and confront the gravity of the hard problem.</p><p><em>Correction: A previous version of the essay&#8217;s introduction stated that most AI use is not deceptive. In fact, 39 of 78 articles in the database are examples of non-deceptive AI use, or 39 out of 74 if we restrict ourselves to political communication and set aside the 4 instances that are scams.</em></p><p><em>This essay is <a href="https://knightcolumbia.org/blog/we-looked-at-78-election-deepfakes-political-misinformation-is-not-an-ai-problem">cross-posted</a> to the Knight First Amendment Institute website. We are grateful to <a href="https://knightcolumbia.org/bios/view/katherine-glenn-bass">Katy Glenn Bass</a> for her feedback.</em></p><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-1" id="footnote-1" target="_self">1</a><div class="footnote-content"><p>The terms mis- and disinformation lack agreed-upon definitions. In this piece, we use the term misinformation to refer to outright false information, as opposed to issues of misleading interpretive framing. Despite many people&#8217;s perception of outgroup narratives as &#8220;misinformation,&#8221; we don't think the misinformation lens is a useful way to think about differences in framing and narratives; we're more narrowly concerned about using outright false information to support those narratives.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-2" id="footnote-2" target="_self">2</a><div class="footnote-content"><p>The low number of total deepfakes found in elections worldwide is surprising on its own terms. The small number could either indicate that AI deepfakes are a much smaller problem so far than anticipated or that the database has many missing entries. Still, other databases that tracked election deepfakes have a similar count for the total number of deepfakes; for example, the German Marshall Fund&#8217;s list of deepfakes related to 2024 elections worldwide has <a href="https://www.gmfus.org/spitting-images-tracking-deepfakes-and-generative-ai-elections">133 entries</a>, though it started collecting entries in September 2023. As we note further along in the essay, the News Literacy Project <a href="https://misinfodashboard.newslit.org/">documented</a> known misinformation about the 2024 elections and <a href="https://newslit.org/newsroom/press-release/the-news-literacy-project-experts-watch-out-for-ai-generated-fakes-and-disinformation-about-voting-ahead-of-election-day/">found</a> that cheap fakes that didn't use AI were used seven times more often than AI-generated content.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-3" id="footnote-3" target="_self">3</a><div class="footnote-content"><p>The dataset also included four instances of AI-generated deepfake videos of politicians used to perpetrate financial scams. Compared to political misinformation, scams have very different dynamics (more sophisticated videos could be more convincing) and stakes (they involve individual financial harm rather than threats to democracy). Similarly, addressing scams requires different interventions&#8212;for instance, monitoring and removing networks of scammers is something major online platforms have been doing for a long time. In other words, scams are a different problem that we have other tools for addressing (regardless of the fact that some platforms arguably underinvest in doing so) and are outside the scope of this essay.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-4" id="footnote-4" target="_self">4</a><div class="footnote-content"><p>In the last legs of the 2024 U.S. election, Google and OpenAI restricted their chatbots from <a href="https://archive.is/EPIcW#selection-975.0-988.0">answering</a> election-related queries&#8212;though competitors like Perplexity didn't, claiming that their product was highly accurate. <a href="https://www.proofnews.org/seeking-election-information-dont-trust-ai/">Evaluating</a> chatbots&#8217; tendency to answer questions factually or abstain from answering questions, improving the factuality of responses, and ensuring chatbots work across different <a href="https://www.proofnews.org/ai-models-falter-answering-election-questions-in-spanish/">languages</a> and contexts are important areas of work as more people turn to chatbots for answering questions.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-5" id="footnote-5" target="_self">5</a><div class="footnote-content"><p>To be clear, we should not treat such propaganda as something newly made possible by AI. It is the incremental evolution of long-standing <a href="https://www.routledge.com/Propaganda-From-Disinformation-and-Influence-to-Operations-and-Information-Warfare/Olejnik/p/book/9781032813721">techniques</a>. Indeed, the cost of creating cartoon avatars for presidential campaigns would be minuscule with or without AI. The impact of propaganda depends not on the technical methods used to create it but rather on the freedom of the press to uplift competing narratives.</p></div></div>