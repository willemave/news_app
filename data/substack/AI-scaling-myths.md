<p>So far, bigger and bigger language models have proven more and more capable. But does the past predict the future?</p><p>One popular view is that we should expect the trends that have held so far to continue for many more orders of magnitude, and that it will potentially get us to artificial general intelligence, or AGI. </p><p>This view rests on a series of myths and misconceptions. The seeming predictability of scaling is a misunderstanding of what research has shown. Besides, there are signs that LLM developers are already at the limit of high-quality training data. And the industry is seeing strong <em>downward</em> pressure on model size. While we can't predict exactly how far AI will advance through scaling, we think there&#8217;s virtually no chance that scaling alone will lead to AGI.&nbsp;</p><h4><strong>Scaling &#8220;laws&#8221; are often misunderstood</strong></h4><p>Research on <a href="https://arxiv.org/abs/2001.08361">scaling laws</a> shows that as we increase model size, training compute, and dataset size, language models get &#8220;better&#8221;. The improvement is truly striking in its predictability, and holds across many orders of magnitude. This is the main reason why many people believe that scaling will continue for the foreseeable future, with regular releases of larger, more powerful models from leading AI companies.</p><p>But this is a complete misinterpretation of scaling laws. What exactly is a &#8220;better&#8221; model? Scaling laws only quantify the decrease in perplexity, that is, improvement in how well models can predict the next word in a sequence. Of course, perplexity is more or less irrelevant to end users &#8212; what matters is &#8220;<a href="https://arxiv.org/abs/2206.07682">emergent abilities</a>&#8221;, that is, models&#8217; tendency to acquire new capabilities as size increases.</p><p>Emergence is not governed by any law-like behavior. It is true that so far, increases in scale have brought new capabilities. But there is no empirical regularity that gives us confidence that this will continue indefinitely.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-1" id="footnote-anchor-1" target="_self">1</a></p><p>Why might emergence not continue indefinitely? This gets at one of the core debates about LLM capabilities &#8212; are they capable of extrapolation or do they only learn tasks represented in the training data? The evidence is incomplete and there is a wide range of reasonable ways to interpret it. But we lean toward the skeptical view. On benchmarks designed to test the efficiency of acquiring skills to solve unseen tasks, LLMs tend to perform <a href="https://arcprize.org/arc">poorly</a>.&nbsp;</p><p>If LLMs can't do much beyond what's seen in training, at some point, having more data no longer helps because all the tasks that are ever going to be represented in it are already represented. Every traditional machine learning model eventually plateaus; maybe LLMs are no different.</p><h4><strong>Trend extrapolation is baseless speculation</strong></h4><p>Another barrier to continued scaling is obtaining training data. Companies are already using all the readily available data sources. Can they get more?</p><p>This is less likely than it might seem. People sometimes assume that new data sources, such as transcribing all of YouTube, will increase the available data volume by another order of magnitude or two. Indeed, YouTube has a remarkable <a href="https://journalqd.org/article/view/4066">150 billion minutes</a> of video. But considering that most of that has little or no usable audio (it is instead music, still images, video game footage, etc.), we end up with an estimate that is much <em>less</em> than the 15 trillion tokens that Llama 3 is already using &#8212; and that&#8217;s before deduplication and quality filtering of the transcribed YouTube audio, which is likely to knock off at least another order of magnitude.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-2" id="footnote-anchor-2" target="_self">2</a></p><p>People often discuss when companies will &#8220;run out&#8221; of training data. But this is not a meaningful question. There&#8217;s always more training data, but getting it will cost more and more. And now that copyright holders have <a href="https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers">wised up</a> and want to be compensated, the cost might be especially steep. In addition to dollar costs, there could be reputational and regulatory costs because society might push back against data collection practices.</p><p>We can be certain that no exponential trend can continue indefinitely. But it can be hard to predict when a tech trend is about to plateau. This is especially so when the growth stops suddenly rather than gradually. The trendline itself contains no clue that it is about to plateau.&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe6b2381-e80e-42b4-b326-814ab6422be7_756x468.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="468" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe6b2381-e80e-42b4-b326-814ab6422be7_756x468.png" width="756" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">CPU clock speeds over time. The y-axis is logarithmic. [<a href="https://en.wikipedia.org/wiki/File:Clock_CPU_Scaling.jpg">Source</a>]</figcaption></figure></div><p>Two famous examples are CPU clock speeds in the 2000s and airplane speeds in the 1970s. CPU manufacturers decided that further increases to clock speed were too costly and mostly pointless (since CPU was no longer the bottleneck for overall performance), and simply decided to stop competing on this dimension, which suddenly removed the upward pressure on clock speed. With airplanes, the story is more complex but comes down to the market prioritizing <a href="https://theicct.org/sites/default/files/publications/Aircraft-fuel-burn-trends-sept2020.pdf">fuel</a> <a href="https://www.etw.de/uploads/pdfs/ATAG_Beginners_Guide_to_Aviation_Efficiency_web.pdf">efficiency</a> over speed.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-3" id="footnote-anchor-3" target="_self">3</a></p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facf7e082-b3c6-4ffb-82c6-85912217135e_2002x1316.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="957" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facf7e082-b3c6-4ffb-82c6-85912217135e_2002x1316.png" width="1456" /><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"><div class="pencraft pc-reset icon-container restack-image"><svg class="lucide lucide-refresh-cw" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg></div><div class="pencraft pc-reset icon-container view-image"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></div></div></a><figcaption class="image-caption">Flight airspeed records over time. The SR-71 Blackbird record from 1976 still stands today. [<a href="https://en.wikipedia.org/wiki/Flight_airspeed_record">Source</a>]</figcaption></figure></div><p>With LLMs, we may have a couple of orders of magnitude of scaling left, or we may <em>already</em> be done. As with CPUs and airplanes, it is ultimately a business decision and fundamentally hard to predict in advance.&nbsp;</p><p>On the research front, the focus has shifted from compiling ever-larger datasets to improving the <a href="https://x.com/karpathy/status/1797313173449764933">quality</a> of training data. Careful data cleaning and filtering can allow building equally powerful models with <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">much</a> <a href="https://arxiv.org/abs/2406.11794">smaller</a> datasets.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-4" id="footnote-anchor-4" target="_self">4</a></p><h4><strong>Synthetic data is not magic</strong></h4><p>Synthetic data is often suggested as the path to continued scaling. In other words, maybe current models can be used to generate training data for the next generation of models.&nbsp;</p><p>But we think this rests on a misconception &#8212; we don't think developers are using (or can use) synthetic data to increase the <em>volume</em> of training data. <a href="https://arxiv.org/html/2404.07503v1">This paper</a> has a great list of uses for synthetic data for training, and it's all about fixing specific gaps and making domain-specific improvements like math, code, or low-resource languages. Similarly, Nvidia's recent <a href="https://developer.nvidia.com/blog/leverage-our-latest-open-models-for-synthetic-data-generation-with-nvidia-nemotron-4-340b/">Nemotron 340B</a> model, which is geared at synthetic data generation, targets alignment as the primary use case. There are a few secondary use cases, but replacing current sources of pre-training data is not one of them. In short, it's unlikely that mindless generation of synthetic training data will have the same effect as having more high-quality human data.</p><p>There are cases where synthetic training data has been spectacularly successful, such as <a href="https://www.nature.com/articles/nature24270">AlphaGo</a>, which beat the Go world champion in 2016, and its successors AlphaGo Zero and <a href="https://www.science.org/doi/10.1126/science.aar6404">AlphaZero</a>. These systems learned by playing games against themselves; the latter two did not use any human games as training data. They used a ton of calculation to generate somewhat high-quality games, used those games to train a neural network, which could then generate even higher-quality games when combined with calculation, resulting in an iterative improvement loop.&nbsp;</p><p>Self-play is the quintessential example of &#8220;System 2 --&gt; System 1 distillation&#8221;, in which a slow and expensive &#8220;System 2&#8221; process generates training data to train a fast and cheap &#8220;System 1&#8221; model. This works well for a game like Go which is a completely self-contained environment. Adapting self-play to domains beyond games is a valuable research direction. There are important domains like code generation where this strategy may be valuable. But we certainly can&#8217;t expect indefinite self-improvement for more open-ended tasks, say language translation. We should expect domains that admit significant improvement through self-play to be the exception rather than the rule.</p><div class="subscription-widget-wrap-editor"><div class="subscription-widget show-subscribe"><div class="preamble"><p class="cta-caption">You&#8217;re reading AI Snake Oil, a blog about our <a href="https://www.aisnakeoil.com/p/ai-snake-oil-is-now-available-to">book</a>. Subscribe to get new posts.</p></div><form class="subscription-widget-subscribe"><input class="email-input" name="email" tabindex="-1" type="email" /><input class="button primary" type="submit" value="Subscribe" /><div class="fake-input-wrapper"><div class="fake-input"></div><div class="fake-button"></div></div></form></div></div><h4><strong>Models have been getting smaller but are being trained for longer</strong></h4><p>Historically, the three axes of scaling &#8212; dataset size, model size, and training compute &#8212; have progressed in <a href="https://www.cnas.org/publications/reports/future-proofing-frontier-ai-regulation">tandem</a>, and this is known to be optimal. But what will happen if one of the axes (high-quality data) becomes a bottleneck? Will the other two axes, model size and training compute, continue to scale?</p><p>Based on current market trends, building bigger models does not seem like a wise business move, even if it would unlock new emergent capabilities. That&#8217;s because capability is no longer the barrier to adoption. In other words, there are many applications that are possible to build with <em>current</em> LLM capabilities but aren&#8217;t being built or adopted due to cost, among other reasons. This is especially true for &#8220;agentic&#8221; workflows which might invoke LLMs <a href="https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful">tens or hundreds of times</a> to complete a task, such as <a href="https://www.youtube.com/watch?v=tNmgmwEtoWE">code generation</a>. </p><p>In the past year, much of the development effort has gone into producing <em>smaller</em> models at a given capability level.<a class="footnote-anchor" href="https://www.aisnakeoil.com/feed#footnote-5" id="footnote-anchor-5" target="_self">5</a> Frontier model developers no longer reveal model sizes, so we can&#8217;t be sure of this, but we can make educated guesses by using API pricing as a rough proxy for size. GPT-4o costs only 25% as much as GPT-4 does, while being similar or better in capabilities. We see the same pattern with Anthropic and Google. Claude 3 Opus is the most expensive (and presumably biggest) model in the Claude family, but the more recent Claude 3.5 Sonnet is both 5x cheaper and more capable. Similarly, Gemini 1.5 Pro is both cheaper and more capable than Gemini 1.0 Ultra. So with all three developers, the biggest model isn&#8217;t the most capable!</p><p>Training compute, on the other hand, will probably continue to scale for the time being. Paradoxically, smaller models require <em><a href="https://arxiv.org/abs/2203.15556">more</a></em><a href="https://arxiv.org/abs/2203.15556"> training</a> to reach the same level of performance. So the downward pressure on model size is putting upward pressure on training compute. In effect, developers are trading off training cost and inference cost. The earlier crop of models such as GPT-3.5 and GPT-4 was under-trained in the sense that inference costs over the model's lifetime are thought to dominate training cost. Ideally, the two should be roughly equal, given that it is always possible to <a href="https://epochai.org/blog/trading-off-compute-in-training-and-inference">trade off</a> training cost for inference cost and vice versa. In a notable example of this trend, Llama 3 used <em>20 times</em> as many training FLOPs for the 8 billion parameter model as the original Llama model did at roughly the same size (7 billion).</p><h4><strong>The ladder of generality</strong></h4><p>One sign consistent with the possibility that we won&#8217;t see much more capability improvement through scaling is that CEOs have been greatly <a href="https://www.cnbc.com/2024/01/16/openais-sam-altman-agi-coming-but-is-less-impactful-than-we-think.html">tamping down</a> AGI expectations. Unfortunately, instead of admitting they were wrong about their naive &#8220;AGI in 3 years&#8221; predictions, they've decided to save face by watering down what they mean by AGI so much that it's meaningless now. It helped that AGI was <a href="https://www.scientificamerican.com/article/what-does-artificial-general-intelligence-actually-mean/">never clearly defined</a> to begin with.</p><p>Instead of viewing generality as a binary, we can view it as a spectrum. Historically, the amount of effort it takes to get a computer to program a new task has decreased. We can view this as increasing generality. This trend began with the move from special-purpose computers to Turing machines. In this sense, the general-purpose nature of LLMs is not new.&nbsp;</p><p>This is the view we take in the <a href="https://www.amazon.com/Snake-Oil-Artificial-Intelligence-Difference/dp/069124913X">AI Snake Oil book</a>, which has a chapter dedicated to AGI. We conceptualize the history of AI as a punctuated equilibrium, which we call the ladder of generality (which isn&#8217;t meant to imply linear progress). Instruction-tuned LLMs are the latest step in the ladder. An unknown number of steps lie ahead before we can reach a level of generality where AI can perform any economically valuable job as effectively as any human (which is one definition of AGI).&nbsp;</p><p>Historically, standing on each step of the ladder, the AI research community has been terrible at predicting how much farther you can go with the current paradigm, what the next step will be, when it will arrive, what new applications it will enable, and what the implications for safety are. <em>That</em> is a trend we think will continue.</p><h4><strong>Further reading</strong></h4><p>A recent <a href="https://situational-awareness.ai/">essay</a> by Leopold Aschenbrenner made waves due to its claim that &#8220;AGI by 2027 is strikingly plausible&#8221;. We haven&#8217;t tried to give a point-by-point rebuttal here &#8212; most of this post was drafted before Aschenbrenner&#8217;s essay was released. His arguments for his timeline are entertaining and thought provoking, but fundamentally an exercise in trendline extrapolation. Also, like many AI boosters, he <a href="https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks">conflates</a> benchmark performance with real-world usefulness.</p><p>Many AI researchers have made the skeptical case, including <a href="https://www.science.org/doi/10.1126/science.ado7069">Melanie Mitchell</a>, <a href="https://x.com/ylecun/status/1796982509567180927">Yann LeCun</a>, <a href="https://garymarcus.substack.com/p/breaking-news-scaling-will-never">Gary Marcus</a>, <a href="https://www.youtube.com/watch?v=UakqL6Pj9xo">Francois Chollet</a>, and <a href="https://arxiv.org/pdf/2402.01817">Subbarao Kambhampati and others</a>.</p><p>Dwarkesh Patel gives a nice <a href="https://www.dwarkeshpatel.com/p/will-scaling-work">overview</a> of both sides of the debate.</p><p><strong>Acknowledgements. </strong>We are grateful to Matt Salganik, Ollie Stephenson, and Benedikt Str&#246;bl for feedback on a draft.</p><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-1" id="footnote-1" target="_self">1</a><div class="footnote-content"><p>Emergent abilities will be predictable if we can find a metric that changes <a href="https://arxiv.org/abs/2304.15004">smoothly</a> instead of discontinuously, but finding such a metric <a href="https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/">isn&#8217;t easy</a>, especially for tasks that require a <a href="https://windowsontheory.org/2023/12/22/emergent-abilities-and-grokking-fundamental-mirage-or-both/">combination</a> of skills. In practice, the question of whether and which new abilities will emerge at the next order of magnitude remains anyone&#8217;s guess.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-2" id="footnote-2" target="_self">2</a><div class="footnote-content"><p>AI companies do use <a href="https://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html">transcribed YouTube data</a> for training, but the reason it is valuable is that it helps LLMs learn what spoken conversations look like, not because of its volume.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-3" id="footnote-3" target="_self">3</a><div class="footnote-content"><p>Libertarian commentators predictably attribute the stagnation of airplane speeds entirely to <a href="https://www.mercatus.org/research/data-visualizations/airplane-speeds-have-stagnated-40-years">regulation</a>, but this is wrong or, at best, highly oversimplified. It&#8217;s true that the FAA essentially banned supersonic flight by civil aircraft over land in the U.S. in 1973. But the fastest aircraft are all military, so the ban doesn&#8217;t affect them. And civil aircraft cruise well below Mach 1 due to fuel efficiency and other considerations.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-4" id="footnote-4" target="_self">4</a><div class="footnote-content"><p>There is a debate about whether LLM training can be made orders of magnitude more sample efficient. After all, children acquire language after being exposed to far fewer words than LLMs are. On the other hand, children are &#8220;<a href="https://www.amazon.com/Scientist-Crib-Early-Learning-Tells/dp/0688177883">scientists in the crib</a>&#8221;, developing world models and reasoning abilities early on, which might be what enables efficient language acquisition. This debate is orthogonal to our point. If task representation or difficulty of extrapolation is the bottleneck, it will represent an upper limit on LLM capabilities regardless of sample efficiency.</p></div></div><div class="footnote"><a class="footnote-number" contenteditable="false" href="https://www.aisnakeoil.com/feed#footnote-anchor-5" id="footnote-5" target="_self">5</a><div class="footnote-content"><p>Even when model developers have released larger models (in terms of parameter count), there is an increased focus on inference efficiency, such as in mixture-of-experts models like <a href="https://mistral.ai/news/mixtral-8x22b/">Mixtral 8x22B</a>, where the number of active parameters during inference is much lower than the total parameter count.</p></div></div>